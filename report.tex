\documentclass[12pt,twoside]{report}
\usepackage{booktabs}       % professional-quality tables
\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% some definitions for the title page
\newcommand{\reporttitle}{Contrastive Medical Image Classification}
\newcommand{\reportauthor}{Shitong Xu}
\newcommand{\supervisor}{Prof. Ben Glocker}
\newcommand{\reporttype}{Meng Individual Report}
\newcommand{\degreetype}{Meng} 

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

% load title page
\begin{document}
\input{titlepage}


% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Your abstract.
\end{abstract}

% \cleardoublepage
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}
% Comment this out if not needed.

% \clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents 


% \clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

Medical image analysis refers to the tasks that train models to perform diagnoses based on visual medical information. Prior works focused on various tasks in this field, including medical image classification and segmentation. These tasks are more challenging than general-propose image processing tasks because specialized knowledge is required to make reasonable predictions. In addition, less data is available due to ethical considerations and high labor requirements to provide ground truth labeling. Based on these restrictions, self-supervised learning algorithms provide promising solutions. \\

Among the prior self-supervised learning methods, contrastive learning has been an effective self-supervised learning method to extract the sample embeddings for downstream tasks. Contrastive learning aims at training an encoder to encode related samples with similar embeddings. A manually defined rule, known as the pretext function, is used to select or generate related samples from the dataset and guide the encoder to encode similar embedding for the selected samples. \\

A wide variety of pretext functions have been proposed before to perform image classification, image-text retrieval, and video recognition tasks. However, limited prior works in the medical image processing field have compared the contrastive learned model's performance with supervised learned models' from the embedding quality's perspective. Comparing embedding quality with supervised learned models helps identify causes for poorly extracted embeddings and encourages novel contrastive learning algorithms being proposed to address the corresponding problem. In addition, most prior works on medical image contrastive learning employed a similar swapped patch prediction pretext task, and lacked exploration on other pretext tasks proposed in the contrastive learning field. We address these two problems by performing detailed experiments and evaluation on embedding quality based on the MedMNIST dataset, and exploring novel attention-masking-based pretext tasks for contrastive learning algorithms. \\


% Different contrastive learning algorithm's embedding performance might differ significantly compared with the result from general visual tasks which works proposed contrastive learning based on, because the medical classification tasks require strong task specific knowledge. As a result, research is required on experimental performance of different existing contrastive learning algorithms on medical image la In addition, limited prior works have proposed investigating more pretext tasks for medical image processing tasks. In this work we propose investigating attention masking's effectiveness as a pretext task for contrastive learning algorithms. 

In conclusion, our research focuses on the following aspects: 
\begin{itemize}
    \item We base on the supervised contrastive learning\cite{supervisedContrastLearning} and compare it with numerous other contrastive learned models' performance on embedding extraction.
    \item We propose a novel attention masking based pretext task as guidance for contrastive learning, and provide a detailed comparison with other pretext tasks proposed in the prior works. 
\end{itemize}

% Medical images are significantly different from general real-world object images, so evaluations done by prior works based on non-medical images might not apply to the medical image embeddings. 


% This machine learning method explores the nature of machine learning models employs a manually defined function as weak-labels of the image. Two encoders are trained to extract the embedding of images, so that the related images have similar embeddings, while non-related images have larger embedding distance. From the prior works, the Contrastive Learning has been proven to be an effective pre-training method in ... tasks. Our work in this project focus on contrastive learning's effectiveness on extracting embeddings to perform medical classification tasks. 

% most of the prior medical image contrastive learning models employed the swapped patch restoration task to perform contrastive learning. The augmented images sufferes from apparent due to patches from different parts of an medical image differes significantly, because they tends to represent different parts of an organ. This might result in the easy training of the 

% \begin{figure}[tb]
% \centering
% \includegraphics[width = 0.4\hsize]{./figures/imperial}
% \caption{Imperial College Logo. It's nice blue, and the font is quite stylish. But you can choose a different one if you don't like it.}
% \label{fig:logo}
% \end{figure}

% Figure~\ref{fig:logo} is an example of a figure. 

\chapter{Ethical Considerations}
The research inevitably involves health data processing, which is collected from real-world patients' personal data. However, this research is exempt from ethical approval as the analysis is based on secondary data which is publicly available, and no permission is required to access the data. In addition, the data are collected anonymously with consent from patients. As a result, there is no risk of the proposed model identifying or tracking the participant. No genetic information processing is involved either since only medical image data is used. \\

The project does not pose threat to human rights due to the following reasons. First, the project aims at training a model to provide effective embedding for medical images, so that further work could base on these high-quality models and perform specific downstream tasks. The trained model and proposed training methods are for academic research use only and do not aim at being used directly in a clinical environment. Secondly, given the limited dataset size and computation resources, the proposed model cannot be directly applied to practical medical diagnosis applications or be subject to misuse. Thirdly, we base our implementation on open-source implementations from prior works, and the proposed attention based pretext task is implemented from scratch. There is no violation of the prior works' copyright. Finally, since the project focuses on image processing, the technology used in the research could theoretically been misused by others to fields including military. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}

\section{Model architectures}
In the computer vision field, Convolutional Neural Networks and Transformer based Neural Networks are commonly used and achieve State-Of-The-Art performances. Both architectures are optimized by gradient descent.

\subsection{Convolutional Neural Network}
In a Convolutional Neural Network (CNN for short), multiple convolution layers and pooling layers are stacked to perceive visual information. In the first several layers of CNN, regional features are extracted to describe simple image patterns. In the later layers, the extracted features are more generalized to depict the whole image's semantic information because each feature value has a higher receptive field. The receptive field of a feature refers to the region in the original image where pixel values influence the feature value. After a generalized image feature is extracted by CNN, the features are flattened to a 1-dimensional feature and pass through multiple fully-connected layers to perform downstream tasks. For a classification task, the last fully connected layer contains the neuron size that corresponds to the category number of the task. Note that with a pre-trained CNN model, the fully connected layers can be trained separately, so that little or no modification is required on the CNN parameters. \\

From the AlexNet\cite{alexnet} to the SOTA CNN models, the models changed from using large kernels to using more layers of smaller convolution kernels. However, when a large number of convolution layers is used, the model suffers from the degradation problem. This is caused by the fact that adding more neural network layers changes the function that the model can represent. As a result, deeper neural networks do not necessarily improve the performance over shallower models, and in some scenarios, show worse performance. To address this problem, ResNet\cite{resnet} proposed by He et al. introduced shortcut connections between networks. In one ResNet layer, the input is directly added elementwise to the layer output, which allows the model to retain the information learned from the previous layers and avoid the degradation problem. The ResNet has been widely used as the backbone model in image processing tasks. 


\subsection{Transformer Neural Network for Computer Vision}
Transformer models\cite{transformer} have shown significant performance in natural language processing\cite{transformer, gpt3, bert} and computer vision tasks\cite{vit, mae}. ViT models proposed by Dosovitskiy et al.\cite{vit} partitioned input images to a sequence of patches before encoding each patch to a low-dimensional embedding using multiple fully connected layers. The list of embedding is appended with a \<CLS\> token at the front before being used as input to a standard transformer encoder model. In a transformer encoder, each input embedding is refined by multiple transformer encoder layers. In one transformer encoder layer, each input embedding first updates its value as the weighted average of other patch's embeddings, and passes through 2 fully-connected layers. The input length and embedding size remain unchanged as the input in each transformer encoder layer. After the last transformer layer, the refined embedding of the \<CLS\> token is considered as the global image feature and used in downstream tasks. 

\section{Contrastive Learning}
Contrastive Learning is a machine learning method that trains an encoder to encode related samples with similar embeddings. A pretext task is defined manually to identify related samples. We refer to the 2 related samples as positive pairs. In some of the implementations, the non-related pairs, which are referred to as negative pairs, are used as well during embedding learning. The pretext task does not have to identify all positive pairs in the dataset but only needs to identify related samples as training guidance to the encoder. For example, classifying adjacent image frames from one video as positive pairs, or image-text pairs from the same website page as positive pairs are all valid pretext tasks, even though both examples ignore other positive pairs in the dataset. In particular, the instance discrimination pretext task is commonly used in prior works on general contrastive learning, which uses different data augmentation methods to generate positive pairs from the same sample. \\

A contrastive learning method can be generalized into 2 steps: 1) Collecting embeddings for each sample as a list and calculating the similarity between every pair of embeddings. 2) Update the encoder to maximize the similarity between positive pair embedding. Based on this framework, He et al.\cite{moco} identified that the large list size and consistency of the list embeddings are 2 important factors for contrastive learning. A large list size ensures each sample's embedding is compared with a large number of non-related samples, so that the embedding learned can have strong generalizability to perform downstream tasks. Consistency means the embedding should be extracted by the same encoder. This property avoids learning embedding with encoder-specific information. Otherwise, the classification step can bypass the sample semantic comparison and base prediction on if embeddings are generated by the same encoder. \\

However, these two properties cannot be satisfied together due to the limit of hardware. Large list size with consistent embedding requires re-extracting sample embedding for all samples in the dataset after each model parameter update. This computation complexity is too high for efficient training. Prior works on solving these two problems can be classified into three categories: Memory-bank based, End-to-end based, and Momentum Contrast based method. 

\subsection{Memory-bank based}
In a Memory-bank based implementation, a list of all samples' embeddings is recorded. After each encoder parameter update, the embeddings of the minibatch samples replace their embedding recorded in the list. This implementation violates the consistency property because only a subset of samples in the list is updated after each encoder update.\\

Wu et al.\cite{instdisc} proposed the memory-bank implementation, and identified the pre-trained classification model's tendency to encode similar images with similar embeddings, even when 2 samples have different ground truth labels. This proves the models are capable of learning similarities between samples without any supervision, which forms the basis of contrastive learning. In addition, the NCE loss is used to approximate the softmax value between all positive and negative pairs' loss with reduced computation complexity. Native contrast loss aims at training the model to classify the positive pair from all sample pairs with the following function: $$ P(x_1, x_2) = \frac{\exp(x_1^T x_2)}{\sum_{i = 1}^{N} x^{nT}_i x^p_1} $$ where $x^p_1$ and $x^p_2$ are positive pair embeddings, $x^n_i$ are negative sample embeddings and $N$ is the total sample size. However, this probability function does not scale with large dataset size because the number of negative pairs grows linearly with the sample size. NCE approximates the softmax value by sampling a subset of $M$ negative pairs from the dataset. The approximate softmax value is then $$ \text{softmax'}(x_1, x_2) = \frac{\exp(x_1^T x_2)}{\frac{N}{M}\sum_{i = 1}^{M} \exp(x^{nT}_i x_1)} \,.$$The probability that sample $x_1, x_2$ are positive pair is calculated by $$ P(x_1, x_2) = \frac{\text{softmax'}(x_1, x_2)}{\text{softmax'}(x_1, x_2) + \frac{M}{N}}\,.$$Tian et al.\cite{cmc} explored the effectiveness of including multimodality data in contrastive learning. Contrastive learned embeddings show constantly improving performance as the number of input modality increases, and achieves comparable performance with supervised learned embeddings. 

\subsection{End-to-end based}
In an End-to-End based implementation, no embeddings extracted in the prior mini-batches are recorded. All positive and negative pairs are sample pairs from the same mini-batch. This implementation ensures the consistency of extracted embeddings because embeddings are re-extracted using the same encoder during each batch training step. However, the list size is limited by the hardware memory size because all the batch samples have to be moved to the computation hardware memory before extracting embeddings. Despite the limit on the list size, most SOTA contrastive learning models are based on this implementation. \\

InvaSpread proposed by Ye et al.\cite{invaspread} employed a list of the same size as batch size. Though it employs a small batch size and does not record any embedding extracted in prior mini-batches, it outperformed its prior work of Instdisc. Oord et al.\cite{cpc} focused on extracting defined positive sample pairs from a temporal signal sequence. In their implementation, embedding extracted from the same real-world signal sequence is considered as positive pairs, and embedding extracted when the encoder is given random noise input forms negative pairs with real-world signal embeddings. InfoNCE is proposed in this work, which further simplified the loss as $$\text{InfoNCE}(x_1, x_2) = \frac{\exp(x_1^T x_2)}{\sum_{i = 1}^{M} \exp(x_i^T x_1)}\,.$$The InfoNCE calculates the contrastive loss with selected $M$ negative samples, and is widely used in other contrastive learning algorithms. SimCLR proposed by Chen et al.\cite{simclr} employed more data augmentation methods for generating positive and negative pairs. A significantly larger batch size is used in SimCLR to address the problem of small list size in End-to-End based training. CLIP proposed by Radford et al.\cite{clip} applies contrastive learning to vision-language multimodal machine learning tasks. The CLIP model shows strong zero-shot performance on 20 datasets. Numerous further works employed the embedding extracted by CLIP to perform multimodal downstream tasks. MA-CLIP proposed by You et al.\cite{maclip} used a similar contrastive training objective as CLIP, and used shared backbone model parameters for different input data modalities. \\

In contrast to increasing the training batch size, clustering-based methods allow efficient end-to-end contrastive learning by training models to predict a category centroid for positive pairs. SwAV by Caron et al.\cite{swav} proposed a swapped prediction problem. For one positive pair, SwAV extracts the embeddings as $z_1, z_2$. The training objective is to predict the embedding of the other sample in the sample pair by using a shared transformation matrix $C$, (i.e. fit the $q_1^T = C z_1$ to $q_2$ and $q_2^T = C z_2$ to $q_1$). Mathematically, the loss function for the swapped prediction problem for one positive pair is $$l(z, q) = - q^T \log(\text{softmax}(C z))$$where the logarithm operation is applied elementwise to the vector. However, this contrastive loss function does not handle the problem of model collapse. A minimum loss value is achieved by the encoder simply output the same embedding for all input, resulting in the failure of model training. As a result, an entropy term is introduced to enhance the clustering performance of the learned embeddings and the Sinkhorn-Knopp algorithm is applied to the $Q$ matrix defined as $Q = [q_1, ..., q_N]$, to ensure different encodings are used for different samples. The resulting contrastive loss for one mini-batch positive pair embeddings $Z_1, Z_2$ is $$L(Z_1, Z_2) = \sum_{n}[l(Z_1^{(n)}, C Z_2^{(n)}) + l(Z_2^{(n)}, C Z_1^{(n)})] + \epsilon H(Q)$$where the $H$ is the entropy function, $Z^{(n)}$ represent one sample embedding of the mini-batch, and $\epsilon$ is a hyperparameter. Finally, SwAV proposed a novel 'multi-crop' data augmentation strategy for positive pair generation. Multi-crop data augmentation generates 2 crops of larger side lengths and V crops with smaller side lengths from the original image. The resulting contrastive loss value across the (V+2) images are calculated by: $$L = \sum_{i \in \{0, 1\}} \sum_{j = 1, j \neq i}^{V+2} L(Z_i, Z_j)\,.$$

\subsection{Momentum Contrast based}
Similar to the Memory-bank based implementation, Momentum Contrast proposed by He et al.\cite{moco} records embeddings extracted in the prior batch iterations and addresses the problem of consistency by momentum updating the encoder parameter. The momentum update ensures there is only a minor difference between the encoder used to extract embeddings in different batches.\\

MoCo\cite{moco} proposed Momentum Contrast for contrastive learning. As shown in Figure \ref{fig:moco}, a pair of encoder models with the same structure are used. Let $x$ be one sample in the mini-batch $B$. $x^q, x_0^k$ are the positive pair generated from the sample $x$, and $\{x_n^k | n \neq 0\}$ are the augmented images generated from other image samples from the mini-batch $B$. To calculate the contrastive loss with respect to sample $x$ in the mini-batch, the query encoder on the left-hand side of Figure\ref{fig:moco} extracts the query embedding from $x^q$ and the key encoder on the right-hand of the Figure extracts the key embedding for all the samples $\{x_n^k\}$. A queue data structure is used to store the key embeddings across different batch training. The InfoNCE loss is used to calculate the contrastive loss between embedding $x^q$ with respect to all embeddings recorded in the queue. Note that the queue size is significantly smaller than the dataset sample size for computation efficiency. When the queue reaches its size limit, the first batch of embeddings added to the queue is replaced. \\

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/moco.png}
  \caption{Momentum Contrastive Learning \cite{moco}}
  \label{fig:moco}
\end{figure}


In addition, MOCO \cite{moco} introduced the shuffled batch normalization, which is used to resolve the information leakage problem caused by the batch normalization layers. The batch normalization layers in the query encoder are updated in each mini-batch calculation. However, including batch normalization layers results in the embeddings learned in the same mini batches sharing batch-specific information. From experiments, the contrastive learned models take advantage of the settings by selecting positive image from embeddings sharing the same batch information. Shuffled batch normalization solved this problem by using more than one GPUs. Each GPU encodes a randomly selected subset samples of the mini-batch. Since the batch normalization layers in different GPUs calculate their batch mean and variance values independently, the model cannot 'cheat' by assuming the key and query embedding share the same batch information. \\

Based on the above MOCO learning method, MoCo-v2 proposed by Chen et al.\cite{mocov2} employed the same training architecture, and explored a different encoder model and data augmentation methods. MoCo-v3 proposed by Chen et al.\cite{mocov3} proved the effectiveness of vision transformers for the Momentum Contrast based implementation. In contrast to MoCo, BYOL proposed by Grill et al.\cite{byol} does not have an explicit data structure for recording the prior embedding extracted. In BYOL implementation, the positive sample pair embeddings $z^p_1, z^p_2$ are extracted by the 2 branches in the BYOL encoding step. Embedding $z^p_1$ then passes through 2 MLP layers to predict the other embedding $z^p_2$. Similar to the SwAV model, BYOL could potentially suffer from the model collapsing problem. However, empirical experiments show that the BYOL performs significantly better than a randomly initialized encoder. Abe Fetterman and Josh Albrecht\cite{explainbyol} examined the reason for the BYOL's resistance to model collapsing, and concluded that the batch normalization layer recorded the feature learned from the prior batches, thus acting as an implicit queue for contrastive learning. 

\section{Contrastive Learning for Medical Image Processing}
Contrastive learning has also been widely applied to medical image processing tasks, including classification and segmentation. In particular, novel pretext functions and training methods are proposed to improve the model's ability to learn distinctive local representations. Based on the different pretext functions used, prior research on contrastive medical image processing could be roughly split into 3 categories: instance level, pixel level, and patch level functions. In this project, MOCO-based and supervised-MOCO-based experiments fall into the category of instance-level contrastive learning, while the new pretext function proposed falls into the category of patch level contrastive learning.

% The model forms a strong data-efficient model for further medical image processing tasks.

% [Self-path: Self-supervision for classification of pathology images with limited annotations] focused on the pathology medical images. Multi-task training is used

\subsection{Instance Level Pretext function}
Instance-level pretext functions construct positive input pairs by employing image data augmentation methods. Models trained using instance-level contrastive learning are optimized to extract embeddings for the whole image. Research that falls into this category mostly directly applied the contrastive learning method explained in the above session and focused on the effectiveness of large pretraining in medical image processing tasks.  \\

Ciga et al. \cite{sslhistopathology} applied SimCLR contrastive pretraining on 57 datasets, and performed experiments on data sampling methods. Their models achieved sota performance compared to the model pretrained using ImageNet dataset, and show that contrastive pretrained models are not capable of extracting features for abstract objectives withouth sample labels, but could benifit from the diversity in pretraining dataset domain to generate higher quality embeddings. Azizi et al.\cite{robustandefficientmedical} performed a large scaled pretraining on datasets across 5 modalities, and showed the model's strong generalizability by adapting the model to numerous other medical image modalities without domain-specific modification. 

\subsection{Pixel Level}
Pixel-level contrastive learning aims at pretraining models to extract features at pixel granularity, which achieves better transfer learning performance on dense prediction tasks (e.g. image segmentation).\\

Wang et al. \cite{denseCL} proposed the DenseCL method, which extend the contrastive loss function to multiple regional feature vector context. In their implementation, each image embedding consists of $S_h \times S_w$ vectors. For each feature vector $v$ in one image embedding $E_1$, the feature vector that forms a positive pair with it from the other image embedding $E_2$ is selected as the feature vector in $E_2$ with the highest cosine similarity. Even though the bootstrapping method constructed its positive pairs using the output of the training model, no unstable training is reported in their experiments. Based on the DenseCL method, Yang et al. \cite{conCL} performed dense contrastive learning by using a pre-trained dense prediction model. In their implementation, the pretrained dense prediction model is used to indicate regions of similar categories. The backbone model is optimized so that it extracts similar embeddings for image regions of the same category. Chen et al.\cite{selfsupervisedlearningformedical} proposed the swapped patch prediction pre-training task. In this task, selected image patches are swapped and models are trained to restore the original image. Swapped patch prediction task is a dense prediction task and does not involve any contrastive learning. However, the perturbation method could be viewed as a pretext function to generate modified images, and widely used by further works in contrastive learning. 

\subsection{Patch Level}
% Despite the success of the instance-level pretext function in extracting global image embeddings, it fails to explicitly train models to focus on regional features. As a result, models trained using instance-level 

Patch-level pretext functions focus on perturbing input or latent embedding patches, thus force model to focus on regional features while extracting global embedding for the whole image. \\

Nguyen et al.\cite{selfsupervisedlearningbasedonspacial} applied the patch swapping pretext function to the 3d image scenario, where 3d volumes are swapped instead of 2d image regions as proposed in the prior work. In addition, the work evaluated the model's performance by investigating the GradCAM attention map. Chaitanya et al.\cite{contrastivelearningofglobalandlocal} employed an encoder-decoder structure, and tackled the problem of learning local features by adding a local contrastive loss term in the original contrastive loss between global image features. To calculate the local contrastive loss, the first $l$ feature map outputs of the decoder are partitioned into feature map volumes. The volumes at the same position across related sample pairs' feature maps are considered positive pairs and are penalized for high differences.

\subsection{Supervised Contrastive Learning}

In contrast to self-supervised prior works, prior works also experimented on involving labeling formation in contrastive learning. Khosla et al.\cite{supervisedContrastLearning} extends the contrastive learning to a supervised context and tackled the problem of reducing embedding distance across more than 2 positive samples. Extensive experiments are used to evaluate the performance of the two proposed loss functions in comparison to the traditional supervised learned models. Ghesu et al.\cite{selfsupervisedlearningfrom100} applied contrastive learning to abnormality detection in chest radiography by using multiple modalities as positive pairs, and trained using a mixed self-supervised and supervised learning loss function. In this research, we also extend MOCO to use labeling information and compare it with the model trained using supervised learning. 

\section{Attention-based Self-supervised Learning} \label{sec:prior_attention_self-supervise}

The attention layer in the transformer model calculates the relative importance, known as the attention, between two local features. Various prior works have tried if the attention information can be used in self-supervised learning to improve the learned embedding quality. \\

Zellers et al. \cite{merlot} proposed the attention-masked language model. In contrast to the masked language modeling method, which randomly masks a certain proportion of language tokens, the attention-masked language model masks the tokens with the higher attention score. This implementation results in significant improvement in the quality of embedding learned by the Transformer encoder model. In addition, Liu et al. \cite{graphAttMask} experimented with adapting attention masking to a Graph Neural Network and trains the model using a contrastive learning method. Their work shows that the attention masking method significantly improves the expressiveness of learned embeddings. The gradient-based MOCO training proposed in this project is also an instance of attention-based self-supervised learning. 

% how to consider multi layer attention
% how to consider score wrt multiple tokens

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Project Plan}
% A Gantt chart for the project is provided in Fig.3.1. In the early stage of implementation, a supervised model based on Supervised Contrastive Learning\cite{supervisedContrastLearning} is trained and used as the baseline model. To achieve a reliable baseline, the model should achieve comparable performance with the classification accuracy reported on the same benchmark dataset. This phase is expected to take 2 weeks. In the next phase lasting around 4 weeks, MoCo, SwAV, and BYOL based models are implemented and checked for validity by comparing them with the phase one baseline. Similar but worse performances are expected compared with the supervised baseline because the models do not have access to the ground truth during the contrastive pre-training step. In the third phase, the embedding learned in the second phase is evaluated as discussed in the Evaluation session. More pretext functions are implemented and evaluated. This phase forms the majority of the research and is expected to take 5 weeks to finish. Instance discrimination, swapped patch prediction, dropout based instance discrimination pretext tasks are implemented and compared. Finally, the proposed attention mask based pretext task is implemented and compared with the pretext tasks identified above. More research is required to define a detailed positive pairs extraction procedure. Thus, this phase requires 6 weeks of developing and evaluating the effectiveness of the proposed pretext function. Finally, the last 3 weeks are reserved for paper drafting. \\

% The backbone models subject to evaluation include ResNet-50 and ViT-B models. The two models spanning both CNN and Transformer categories are widely used in prior works in the contrastive learning field. In addition, the novel attention masking pretext we propose is based on the prior work MERLOT\cite{merlot}, which employed a ViT based model. As a result, evaluations on the ViT based models are an indispensable part of the research. 

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{figures/individual-project-gantt.drawio.png}
%     \caption{Project Gantt Chart}
%     \label{fig:my_label}
% \end{figure}

\chapter{Methods}
\section{Problem Statement}
Pathology medical images are images that detail specific cell features for diagnosing related diseases. In this research, we based experiments on the Pathology dataset in the MedMNIST benchmark. Each sample image in the dataset is a 2D RGB image of size $28 \times 28$ pixels. The Pathology dataset consists of images from 9 classes. Figure \ref{fig:examples} shows randomly sampled images from each image category. For simplicity, we refer to the normal colon mucosa as NCM, cancer-associated stroma as CAS, colorectal adenocarcinoma epithelium as CAE throughout the rest of the project. \\

\begin{figure}
    \centering
    \includegraphics{figures/examples.png}
    \caption{Example images from the Pathology dataset}
    \label{fig:examples}
\end{figure}

The Pathology dataset from the MedMNIST benchmark provides a split for training, development, and testing datasets. In our experiments, we further split the training dataset into the pretraining dataset and pretrain-validation dataset. The pretraining dataset consists of 80996 samples, and is used as the pretraining dataset for MOCO-based learning. The pretrain-validation dataset is used to calculate contrastive loss, in order to verify if the model overfitted in the pretraining stage. Note that since supervised MOCO is not a self-supervised learning method, the pre-training dataset is used for both the backbone model training and the classification head training. The development dataset is split into the dev-train and dev-validation datasets. The dev-train dataset consists of 9003 samples, and is used to train the linear classification head on the embeddings extracted by self-supervised learned backbone models. The dev-validation dataset is used to verify if the linear classifier has overfitted to the dev-train dataset. The test set provided by the Pathology dataset remains unchanged, and is used to record metric scores (e.g. F1 score) and visualize embedding distributions via PCA or t-SNE. In addition to the MOCO-based pretrained model, we trained a Resnet-50 model end-to-end on both the training set and the development set, as explained in Sec \ref{sec:naive_supervised}. Table \ref{tab:dataset_scale} and Table \ref{tab:dataset_usage} summarize the scale and usage of each dataset. \\

\begin{table}[]
    \centering
    \begin{tabular}{|l|l|l|}
    \toprule
    provided dataset & dataset after split & size \\
    \hline
    training set & pretrain set & 80996\\
                 & pretrain-validation set & 9000 \\
    \hline
    development set & dev-train set & 9003 \\
                 & dev-validation set & 1001 \\
    \hline
    test set     & test set & 7180 \\
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{Scale of different datasets}
    \label{tab:dataset_scale}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{|p{2.5cm}|p{2.6cm}|p{2.5cm}|p{2.1cm}|p{2.3cm}|}
    \toprule
    dataset & naive MOCO \& gradient-based MOCO training & supervised MOCO training & training set end-to-end training & development set end-to-end training\\
    \hline
    pretrain set   & encoder        & encoder \&     & training   & not used \\
                   & pretraining    & classification &            &          \\
                   &                & head training  &            &          \\
    \hline
    pretrain       & encoder        & validation     & validation & not used \\
    validation set & validation     &                &            &          \\
    \hline
    dev-train set  & classification & not used       & not used   & training \\
                   & head training  &                &            &          \\
    \hline
    dev-           & classification & not used       & not used   & validation \\
    validation set & head           &                &            &            \\
                   & validation     &                &            &            \\
    \hline
    test set & test & test & test & test \\
    
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{Usage of different datasets}
    \label{tab:dataset_usage}
\end{table}

After applying the random split as explained above, the number of samples belonging to each class is shown in Table \ref{tab:dataset_balance}. The percentage in each cell shows the sample size of the class with respect to the sample size of the whole dataset. In general, training and validation datasets generated from the same source datasets share similar relative class sizes. The test set has significantly different relative class sizes from the training and development sets. \\

\begin{table}[]\ 
    \centering
    \addtolength{\leftskip} {-2cm}
    \addtolength{\rightskip}{-2cm}
    \begin{tabular}{|l|l|l|l|l|l|}
    \toprule
    class name & pretrain & pretrain-vali & dev-train & dev-vali & test \\
    \hline
    adipose & 8425 (10.4 \%) & 941 (10.5 \%) & 939 (10.4 \%) & 102 (10.2 \%) & 1338 (18.6 \%) \\
    background & 8589 (10.6 \%) & 920 (10.2 \%) & 955 (10.6 \%) & 102 (10.2 \%) & 847 (11.8 \%) \\
    debris & 9355 (11.5 \%) & 1005 (11.2 \%) & 1037 (11.5 \%) & 115 (11.5 \%) & 339 (4.7 \%) \\
    lymphocytes & 9357 (11.6 \%) & 1044 (11.6 \%) & 1049 (11.7 \%) & 107 (10.7 \%) & 634 (8.8 \%) \\
    mucus & 7235 (8.9 \%) & 771 (8.6 \%) & 804 (8.9 \%) & 86 (8.6 \%) & 1035 (14.4 \%) \\
    smooth muscle & 10952 (13.5 \%) & 1230 (13.7 \%) & 1211 (13.5 \%) & 143 (14.3 \%) & 592 (8.2 \%) \\
    NCM & 7076 (8.7 \%) & 810 (9.0 \%) & 786 (8.7 \%) & 91 (9.1 \%) & 741 (10.3 \%) \\
    CAS & 8448 (10.4 \%) & 953 (10.6 \%) & 939 (10.4 \%) & 106 (10.6 \%) & 421 (5.9 \%) \\
    CAE & 11559 (14.3 \%) & 1326 (14.7 \%) & 1283 (14.3 \%) & 149 (14.9 \%) & 1233 (17.2 \%) \\
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{Percentage of different class samples in each datasets. Pretrain-vali stands for the pretrain validation set, and dev-vali stands for the dev-validation set}
    \label{tab:dataset_balance}
\end{table}

The prior work from Ciga et al. \cite{sslhistopathology} explored the effectiveness of colour jittering and random resize cropping augmentation methods on pathology image datas in the contrsative learning context. Similarly, the MOCO-v2 \cite{mocov2} used the same augmentation methods. As a result, in the following experiments, unless otherwise specified, the augmentations used are all: Random resize cropping sample images to $224 \times 224$ image size, followed by randomly applying colour jittering with probability 0.8, grayscaling with probability 0.2, gaussian blurring with probability 0.5, and horizontal flipping with probability 0.5. 
% Figure... shows the output image after each augmentation step

\section{Contrastive Pretraining}
\subsection{Naive MOCO training} \label{sec:naiveMOCO}
We performed MOCO-v2 \cite{mocov2} learning on the pretraining dataset. We selected MOCO based on the following considerations: Contrastive learning's effectiveness relies on the large mini-batch size, which allows the model to compare the embedding extracted for each sample to a large number of other sample images. This helps the model learn from the difference between positive and negative sample pairs, thus improving model generalizability. However, the available GPUs for the project have small memory sizes that limit the batch size to below 128 images. Other contrastive learning methods, e.g. SimCLR, used a mini-batch size of 8192, which is significantly larger than the maximum batch size allowed by the project. In contrast, MOCO stores a list of history embeddings extracted from the previous mini-batches. This removes the requirement of having a large batch size, and thus is expected to achieve better performance than the other contrastive learning methods under the same hardware limitation.\\

The model used in pretraining is a Resnet-50 model. Following the work proposed by Chen et al. \cite{mocov2} and evaluation performed by Bordes et al. \cite{guillotine} and Balestriero et al. \cite{cookbook}, we removed the MLP layers after the convolution layers once pretraining is finished and used the max-pooled output of conv layers as the learned embeddings. The trained backbone model then extracts embeddings for the whole dev dataset. The extracted dev-training dataset embeddings are constructed as a new dataset and used to train the classification head. The linear layer projects the 2048 dimension feature extracted by the previous Conv layers to 9 dimension space corresponding to the number of image classes contained in the Pathology dataset. In order to achieve a fair comparison, unless otherwise specified, all the MOCO-based training methods explored in this project use the same model architecture as explained above. We refer to the model as the modified Resnet-50 model. Apart from the loss function and pretext function used, all the MOCO-based training methods also use the same training procedure as the naive MOCO training.\\

In the native MOCO pretraining experiments, 2 GPUs are used with a batch size of 112 on each GPU. In the pretraining stage, it takes around 5 hours to pretrain the backbone model for 20 epochs. In the final reported results, we scaled up the training time to 120 epochs and repeat experiments for 3 times. The scaled-up experiments take 30 hours to train on 2 Tesla T4 GPUs. 

% In the following MOCO-based learning methods, unless otherwise specified, the hyperparameter tuning stage all last for 20 epochs, and scaled up experiments are all . 

\subsection{GradCAM-based MOCO training}
As introduced in Sec \ref{sec:prior_attention_self-supervise}, attention masking has shown strong performance in various self-supervised learning contexts. In this project, we experimented on if masking the region where the model has high attention could be in a strong pretext function. Unlike the ViT model, the modified Resnet-50 model we base on uses a CNN encoder model, which does not calculate attention score in the forward pass. As a result, we instead extract the attention information using Grad-CAM. \\

% The gradient map is used as the attention map to identify the region where the model showed the most attention.

% defining the pretext function based on the region where model showed the highest attention 

% information selected the latend information 


The GradCAM-based MOCO differs from the naive MOCO only from the pretext function used. The procedure of the new pretext function is shown in Figure\ref{fig:grad_perturb_explain}. Images A and B form a positive pair generated from the same sample image by data augmentation. In each mini-batch training step, a contrastive loss is first calculated between images A and B. Then GradCAM feature map is generated to locate the image region that has a ... influence on the contrastive loss. Finally, image B is divided into $n \times n$ patches. The patch where the largest GradCAM value locates is masked with zero values. The masked image forms a positive pair with the original unmasked image, and is used to calculate new contrastive loss. The query encoder model is optimized using the second contrastive loss. Same to the MOCO training, InfoNCE is used in both loss values' calculations. The code used for generating masked images is listed below. 

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/gradcam_explain.png}
  \caption{Procedure of the GradCAM-based pretext function}
  \label{fig:grad_perturb_explain}
\end{figure}

\begin{lstlisting}[language=Python, caption=Gradient-based Masking function]
    def GradCAM_based_mask(img, grad, h, w):
      '''
      The function that masks the image region where 
      the maximum GradCAM value locates
    
      Input:
      - img, gradcam: shape: (batch_size, 3, H, W)
      - h, w: number of patches along each side of the image, 
              H / h and W / w must be integers
    
      Output: 
      - shape: imgs of shape: (batch_size, 3, H, W)
      '''
    
      b, c, H, W = img.shape
      patch_size = (int(H / h), int(W / w))
      
      # group GradCAM belonging to the same patch together 
      patches = nn.functional.unfold(
                    gradcam.sum(dim=1, keepdim=True), 
                    kernel_size=patch_size, 
                    stride=patch_size)

      # create 2D mask empty and fill the masked position with 1
      mask = torch.zeros((b, patches.shape[-1])).to(img.device)
      mask = mask.scatter(1, 
                patches.sum(dim=1).argmax(dim=-1)[:, None], 1)

      # apply mask to the image
      mask = mask[:, None, :]
                .repeat_interleave(patches.shape[1], dim=1)
      mask = nn.functional.fold(
                mask, 
                (H, W), 
                kernel_size=patch_size, 
                stride=patch_size)
      img = img.masked_fill(torch.gt(mask, 0), 0)
      
      return img
\end{lstlisting} \label{fig:masking_code}

Using the above image masking function, the following code added to each mini-batch training constructs a modified image as input for the second contrastive loss calculation. 

\begin{lstlisting}[language=Python, caption=additional training code for using ]
    # ... code for loading this mini-batch samples ...
    model.eval()
    
    # calculate the first contrastive loss
    images[0] = images[0].clone().detach()
    output, target = model(im_q=images[0], 
                           im_k=images[1], 
                           labels=labels)
    loss = criterion(output, target)
    loss.backward()

    # generate GradCAM feature map that identifies the most 
    # attended region with high values
    gradcam = ...

    # construct masked image
    # the example splits image into 8x8 = 64 patches
    images[0] = gradient_based_mask(images[0], 
                                    gradcam, 
                                    h=8, w=8)
    images[0] = images[0].detach()
    optimizer.zero_grad()

    model.train()
    
    # ... the rest of the training code ...
\end{lstlisting}

This perturbation method is expected to result in better performance than the model trained using the naive MOCO implementation. The region with a ... GradCAM value indicates that the region has a ... influence on the contrastive loss. Masking the region with zeros removes the context where the model base prediction on, Masking the image region with zeros ... removes the context provided to the model, forces the model to also attend to the features in other regions of the image, and thus improve the quality of embedding learned. \\

It takes 8 hours to train the model using Gradient based perturbation for 20 epochs on the pretraining dataset. The increased computation cost is caused by the additional inference and backpropagation step required to calculate the Grad-CAM feature maps for images in the mini-batch. 

\section{Supervised Learning}
\subsection{End-to-end supervised learning} \label{sec:naive_supervised}
We trained the modified Resnet-50 model end-to-end on both the pretraining dataset and the dev-train dataset. These experiments are used as a performance baseline for both our supervised and self-supervised experiments. In order to achieve a fair comparison, we used the same modified Resnet-50 model as introduced in Section \ref{sec:naiveMOCO}. \\

We performed experiments on one Tesla T4 GPU with a batch size of 128. The training on the pretraining dataset takes 35 epochs before reaching convergence, and the training on the dev-training dataset takes 160 epochs. Both of these training configurations take around 5 hours to complete. 

\subsection{Training MOCO based supervised model}
Following the prior work proposed by Ghesu et al. \cite{selfsupervisedlearningfrom100}, we also evaluated supervised MOCO pretraining's embedding distribution and compared them with that of the model trained end-to-end on the pretraining dataset.\\

In the supervised MOCO, positive image pairs are images that have the same labeling in the dataset, and negative pairs are images with different labels. The loss value of one mini-batch is defined as:
$$L = \sum_{x_i^q \in B, x_j^k \in S, C_{x_i^q} = C_{x_j^k}}\log(\frac{\exp(x_i^q \cdot x_j^k)}{\sum_{x^q \in B, x^k \in S}\exp(x^q \cdot x^k)})$$
, where the $B$ represents the set of query embeddings extracted from the current mini-batch, $S$ represents the set of key embeddings recorded from the current and the previous mini-batchs, $x_i^q, x_j^k$ represent the query and key embedding extracted from image $x_i, x_j$, and $C_{x_i^q}, C_{x_j^k}$ represent the label associated with image $x_i, x_j$ respectively. Figure \ref{fig:supervisedMOCO_loss_explain} explains the loss function visually using the same notation.\\

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/supervisedMOCO_explain.png}
    \caption{Supervised MOCO loss function explanation}
    \label{fig:supervisedMOCO_loss_explain}
\end{figure}

The training is performed on 2 GPUs. The training hardware used and time cost is the same as the model trained using naive MOCO training explained in Sec \ref{sec:naiveMOCO}. 

\chapter{Evaluation}
In this section, we detail the hyperparameter tuning experiments and the evaluations of the embedding qualities learned by each training method. In particular, the F1, precision, recall scores, and classification loss values are calculated for qualitative evaluation. t-SNE and PCA are used to visualize the distribution of embedding learned. \\

In order to achieve a fair comparison, the classification heads of naive MOCO and Gradient-based perturbation MOCO are both trained using the Adam optimizer for 160 epochs on dev-training dataset with a learning rate of 0.01. The supervised MOCO trains the classification head using the Adam optimizer for 40 epochs on the pretraining dataset. All of the classification heads' training take less than 15 minutes to complete on a single Tesla T4 GPU. \\

We first present the end-to-end supervised method's performance in Section \ref{sec:end2end}. Then we use the recorded performance from end-to-end supervised training as a baseline for comparison in the other 3 training methods in Section \ref{sec:naiveMOCO_eval}, \ref{sec:supervisedMOCO_eval}, and \ref{sec:gradientMOCO_eval}. 

\section{End-to-end supervised method} \label{sec:end2end}
In this section, we present the performance of the model trained end-to-end on the pretraining and dev-training datasets. The default hyperparameters used for training are shown in Table \ref{tab:supervised_hyperparameters}, and we tuned the constant learning rate used for both models trained on each dataset. \\

\begin{table}[]
    \centering
    \begin{tabular}{ll}
    \toprule
    Hyperparameter & Value \\
    \midrule
    SGD momentum & 0.9 \\
    SGD weight-decay & 1e-4 \\
    Learning rate scheduling & constant learning rate \\
    \bottomrule
    \end{tabular}
    \caption{Default hyperparameters used in end-to-end training}
    \label{tab:supervised_hyperparameters}
\end{table}

We trained the modified Resnet-50 model end-to-end on the pretraining dataset. Table \ref{tab:large_supervised_tuning} shows the means training and validation loss of the models. The model trained with 0.1 learning rate early stopped at the 35th epoch, and achieved the lowest final validation loss. We repeated the experiment 4 times and report the optimal model performance in Table \ref{tab:naive_supervised_final}. In addition, the elementwise average of the confusion matrix, precision values, and recall values are shown in Figure \ref{fig:large_supervised_cm} and Table \ref{tab:large_supervised_preci_recall}. The CAS images category has a significantly lower recall score than the other classes, with around half of the CAS samples being misclassified as debris and smooth muscle images. The debris image category has the lowest precision score. Most images that are misclassified as debris images come from the CAS category. On the contrary, the mucus image category, which has the smallest sample size in the pretraining dataset, does not show either low precision or low recall values. In the following sessions, unless otherwise specified, all the confusion matrices and metric tables report the elementwise averages of the scores generated from repeated experiments.\\

% As shown in Table ..., both of these image classes have significantly smaller sample sizes in the test set than in the training and development set. 

% In addition, we further scaled up experiments to perform 40 epochs of training. As shown in Figure \ref{fig:large_training_curve}, the model validation loss further decreased to 0.2779 before showing an overfitting trend. As a result, we early stop at the 35th epoch, and report the optimal model performance in Table \ref{tab:naive_supervised_final}. 

% As shown in Figure\ref{fig:large_supervised_embedding}, based on the above hyperparameters, we visualize the embedding distribution learned by the model trained on the pretraining dataset using t-SNE and PCA. The model selected for visualization achieved a 0.8828 F1 score. 

\begin{table}[]
    \centering
    \begin{tabular}{lllll}
    \toprule
    LR & Training Loss & Validation Loss \\
    \midrule
    0.05 & $0.2627$ & $0.3385$\\
    0.1 & $0.3032$ & $0.3311$\\
    0.3 & $0.3104$ & $0.3651$\\
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{Learning rate hyperparameter tuning for end-to-end training on the pretraining dataset}
    \label{tab:large_supervised_tuning}
\end{table}

% \begin{figure}
%     \centering
%     \includegraphics[width=.9\linewidth]{figures/large_training_curve.png}
%     \caption{Learning curves of end-to-end supervised training on the pretraining dataset. The model overfits after 35 epochs}
%     \label{fig:large_training_curve}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/large_supervised_cm.png}
    \caption{Confusion Matrix of model trained end-to-end on the pretraining dataset.}
    \label{fig:large_supervised_cm}
\end{figure}

\begin{table}[]
    \centering
    \begin{tabular}{llll}
\toprule
 & adipose & background & debris \\
\hline
Precision & $0.9810 \pm 0.0029$ & $0.9682 \pm 0.0141$ & $0.7202 \pm 0.0626$ \\
Recall & $0.9850 \pm 0.0026$ & $1.0000 \pm 0.0000$ & $0.9041 \pm 0.0153$ \\
\toprule
 & lymphocytes & mucus & smooth muscle \\
\hline
Precision & $0.9445 \pm 0.0024$ & $0.9912 \pm 0.0017$ & $0.7906 \pm 0.0397$ \\
Recall & $0.9795 \pm 0.0018$  & $0.9312 \pm 0.0067$ & $0.9086 \pm 0.0176$ \\
\toprule
 & NCM & CAS & CAE \\
\hline
Precision & $0.9271 \pm 0.0177$ & $0.7983 \pm 0.0158$ & $0.9275 \pm 0.0262$ \\
Recall & $0.9201 \pm 0.0376$ & $0.4690 \pm 0.0192$ & $0.9297 \pm 0.0155$ \\
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{Per-class precision and recall values of end-to-end trained model on the pretraining dataset.}
    \label{tab:large_supervised_preci_recall}
\end{table}



In order to visualize the embedding learned by the model end-to-end trained on the pretraining dataset, we select one model that achieved 0.8828 F1 score on the test set. The t-SNE and PCA visualizations of the embedding distribution are shown in Figure \ref{fig:large_supervised_embedding}. \\


\begin{figure}
\centering
    \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=1.0\linewidth]{figures/73909.png}
      \caption{t-SNE embedding visualization}
      \label{fig:large_supervised_tsne}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=1.0\linewidth]{figures/73909_pca.png}
      \caption{PCA embedding visualization}
      \label{fig:large_supervised_pca}
    \end{subfigure}
    \caption{Embedding visualizations of the model trained end-to-end on the pretraining dataset}
    \label{fig:large_supervised_embedding}
\end{figure}

Models trained end-to-end on the dev-training dataset with different learning rates are shown in Table \ref{tab:small_supervised_tuning}. Due to the smaller sample size, the model achieves lower scores than the model trained on the larger pretraining dataset as expected. We select 0.005 learning rate as our final hyperparameter configuration due to its smaller final validation loss. \\

Based on the above observations on learning rates, we scaled up the learning to 320 epochs. However, as shown in Figure \ref{fig:small_training_curve}, the model trained on the dev-train dataset does not show further improvement in the validation loss value. As a result, we early stop training at 160 epochs and report the average performance of 3 experiments in Table \ref{tab:naive_supervised_final}. In addition, Figure \ref{fig:small_supervised_cm} and Table \ref{tab:small_supervised_preci_recall} show the confusion matrix, mean precision scores, and mean recall scores. Similar to the confusion matrix of the model trained end-to-end on the pretraining dataset, the CAS image category has the lowest recall score. More than half of the CAS images are misclassified as the smooth muscle images. The smooth muscle image category has the lowest precision score even though it has the second-largest sample size in the dev-training dataset. \\

\begin{table}[]
    \centering
    \begin{tabular}{lllll}
    \toprule
    LR & Training Loss & Validation Loss \\
    \midrule
    0.001 & $0.2628$ & $0.4253$ \\
    0.005 & $0.2723$ & $0.4208$ \\
    0.01  & $0.2962$ & $0.4376$ \\
    0.1   & $0.3445$ & $0.6037$ \\
    \bottomrule
    \end{tabular}
    \caption{learning rate hyperparameter tuning results for end-to-end supervised training on the development dataset}
    \label{tab:small_supervised_tuning}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figures/small_training_curve.png}
    \caption{Learning curves of end-to-end supervised training on the dev-train dataset. The training converges after 160 epochs}
    \label{fig:small_training_curve}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/small_supervised_cm.png}
    \caption{Confusion matrix of the model trained end-to-end on the dev-training dataset}
    \label{fig:small_supervised_cm}
\end{figure}

\begin{table}[]
    \centering
    \begin{tabular}{llll}
\toprule
 & adipose & background & debris \\
\hline
Precision & $0.9677 \pm 0.0170$ & $0.9524 \pm 0.0130$ & $0.7151 \pm 0.0107$ \\
Recall & $0.9671 \pm 0.0035$ & $1.0000 \pm 0.0000$ & $0.8761 \pm 0.0715$ \\
\toprule
 & lymphocytes & mucus & smooth muscle \\
\hline
Precision & $0.8601 \pm 0.0631$ & $0.9784 \pm 0.0001$ & $0.7189 \pm 0.0300$ \\
Recall & $0.9834 \pm 0.0027$ & $0.8978 \pm 0.0274$ & $0.9146 \pm 0.0107$ \\
\toprule
 & NCM & CAS & CAE \\
\hline
Precision & $0.9448 \pm 0.0313$ & $0.7457 \pm 0.0672$ & $0.8819 \pm 0.0449$ \\
Recall & $0.8218 \pm 0.0289$ & $0.3262 \pm 0.0742$ & $0.9106 \pm 0.0376$ \\
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{Per-class precision and recall values of end-to-end trained model on the dev-training dataset.}
    \label{tab:small_supervised_preci_recall}
\end{table}

We select one model trained using 0.01 learning rate that achieved 0.8361 F1 score for t-SNE and PCA embedding distribution visualization. In comparison to the embedding distribution learned by the model trained on the pretrain dataset, the model trained on the dev-training dataset is less capable of encoding cancer-associated stroma and colorectal adenocarcinoma epithelium cells with different embeddings. No significant difference is shown in PCA embedding visualization. \\

\begin{figure}
\centering
    \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=1.0\linewidth]{figures/74198.png}
      \caption{t-SNE embedding visualization}
      \label{fig:small_supervised_tsne}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=1.0\linewidth]{figures/74198_pca.png}
      \caption{PCA embedding visualization}
      \label{fig:small_supervised_pca}
    \end{subfigure}
    \caption{Embedding visualizations of the model trained end-to-end on the dev-training dataset}
    \label{fig:small_supervised_embedding}
\end{figure}


% We didn't perform learning rate scheduling due to the following consideration. As shown in Figure\ref{fig:large_training_curve} and Figure\ref{fig:small_training_curve}, higher learning rates only result in slightly faster convergence speed in the earlier epochs and tend to converge to the local optimum at the end of training. As a result, little improvement in convergence speed is expected from a learning rate scheduling is expected to little improvement from no learning rate scheduling is performed for both models' training. \\

\begin{table}[]
    \centering
    \begin{tabular}{llllll}
    \toprule
    dataset & LR & F1 & Precision & Recall & Test Loss \\
    \midrule
    pretraining & 0.1 & $0.8862 \pm 0.0047$ & $0.8943 \pm 0.0011$ & $0.8919 \pm 0.0044$ & $0.2533 \pm 0.0021$ \\
    dev-training & 0.005 & $0.8376 \pm 0.0100$ & $0.8437 \pm 0.0045$ & $0.8488 \pm 0.0166$ & $0.3379 \pm 0.0510$ \\
    \bottomrule
    \end{tabular}
    \caption{Final performance of end-to-end supervised trained model}
    \label{tab:naive_supervised_final}
\end{table}


% On visualizing samples of both categories of images, both categories of images do show strong similarity. 
In the following supervised experiments, we compare the performance of supervised MOCO (Section \ref{sec:supervisedMOCO_eval}) with the model trained by end-to-end supervised method on the pretraining dataset, and compare the performance of naive MOCO (Section \ref{sec:naiveMOCO_eval}) and gradient-based perturbation MOCO (Section \ref{sec:gradientMOCO_eval}) with the model trained on the dev-training dataset. 

% This section reports the hyperparameter tuning result of the model trained on the pretraining dataset. We experimented with learning rates in range .... As shown in Fig. ..., the learning rate of ... showed the best performance. 

% As a result, the learning rate of ... is selected to run for more epochs. The learning curve of .. epochs experiments is shown in Fig. ... 

\section{Naive MOCO training} \label{sec:naiveMOCO_eval}
Table \ref{tab:naive_moco_hyperparameters} shows the default hyperparameters proposed in MOCO-v2 \cite{mocov2}. Based on these hyperparameters, we tuned the learning rate for naive MOCO training. In particular, we applied cosine learning rate annealing, which consists of several learning rate decaying stages. In each stage, the learning rate is set to an initial high value at the beginning of the stage, and then gradually decreased to a low value. All annealing stages share the same initial learning rate, which is tuned in the following experiments. The black line in Figure \ref{fig:naiveMOCO_pretrain_curve} and the gray line in Figure \ref{fig:naiveMOCO_pretrain_curve_final} show the learning rate used in each epoch when an initial learning rate of 0.1 is used. \\
% Cosine learning rate scheduling helps prevent the trained model from converging to the local optimum. 

\begin{table}[]
    \centering
    \begin{tabular}{ll}
    \toprule
    Hyperparameter & Value \\
    \midrule
    SGD momentum & 0.9 \\
    SGD weight-decay & 1e-4 \\
    Initial Learning rate & 0.03 \\
    Learning rate scheduling & cosine learning rate schedule \\
    MOCO momentum & 0.999 \\
    MOCO softmax temperature & 0.07 \\
    \bottomrule
    \end{tabular}
    \caption{Default hyperparameters proposed in MOCO-v2}
    \label{tab:naive_moco_hyperparameters}
\end{table}

Figure \ref{fig:naiveMOCO_pretrain_curve} shows the contrastive loss values of naive MOCO pretrained for 20 epochs. Model trained using a learning rate of 5.0 results in unstable training, and failed to converge to an optimum solution. When setting the learning rate to a small value, e.g. 0.005, the model converges at a significantly lower speed, as shown by the blue line in the figure. Setting learning rates in $\{0.05, 0.1, 0.5\}$ results in insignificant differences in the training and validation loss curves, as shown by the standard deviation (the light orange region) around the mean loss values (the orange curve). As a result, we select the median value 0.1 among $\{0.05, 0.1, 0.5\}$ as the learning rate used in further experiments. The training and validation loss curves of 0.1 learning rate are shown as the green lines in Figure \ref{fig:naiveMOCO_pretrain_curve}. \\

\begin{table}[]
    \centering
    \begin{tabular}{lll}
    \toprule
    Learning Rate & Training Contrastive Loss & Validation Contrastive Loss \\
    \midrule
    $0.005$ & $6.6337$ & $6.5222$ \\
    $\{0.05, 0.1, 0.5 \}$ & $6.4500 \pm 0.1003$ & $6.4000 \pm 0.1010 $ \\
    $0.1$ & $6.462$ & $6.4062$ \\
    $1.0$ & $6.2764$ & $6.2165$ \\
    $5.0$ & $7.8421$ & $7.7641$ \\
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{Learning rate hyperparameter tuning results for naive MOCO training}
    \label{tab:naive_moco_tuning}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figures/moco_pretrain_curve.png}
    \caption{Learning rate hyperparameter tuning for naive MOCO training}
    \label{fig:naiveMOCO_pretrain_curve}
\end{figure}

In addition, we notice that a learning rate of 1.0 shows slightly better final validation and training contrastive losses than that of the learning rates in range [0.05, 0.5]. As a result, we performed more epochs of experiments on both learning rates of 0.1 and 1.0. As shown in Figure \ref{fig:naiveMOCO_pretrain_curve_final}, in the second cosine annealing stage, 1.0 learning rate results in a significant increase in standard deviation in the validation loss values, but showed little fluctuation in the training loss. We conclude that the model trained with 1.0 initial learning rate overfitted to the pretraining dataset during the first 20 epochs of training, thus a minor subsequent change in model parameters will result in a significant increase in validation losses. In contrast, 0.1 learning rate results in a stably decreasing learning curve in each annealing stage throughout the 120 epochs of training, and thus is selected as the final learning rate configuration. \\

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figures/moco_pretrain_curve_final.png}
    \caption{Scaled-up experiments on naive MOCO training}
    \label{fig:naiveMOCO_pretrain_curve_final}
\end{figure}

We repeated the 120 epochs' experiment 3 times and plot the mean and standard deviations of the loss values, as shown by the blue line and light blue region in Figure \ref{fig:naiveMOCO_pretrain_curve_final}. In addition, as shown in Table \ref{tab:MOCO_final_metric}, we report the mean metric performance achieved by the linear classification models trained on the features extracted by these pretrained encoders. Though the naive MOCO pretrained the encoder model on additional data, i.e. the pretraining dataset, the classification performance is only marginally better than the model trained end-to-end on the dev-training dataset.\\

We further explore if the low complexity of the linear classification head limited the model performance. As shown in Table \ref{tab:MOCO_multimlp}, the model classification performance only improved slightly after adding a 1024 neuron layer, and worsen when different classification heads are used. We conclude that either the other hyperparameters untuned limited the model's performance, or the pretraining dataset is too small for the pretrained encoder to outperform the baseline model significantly. \\


\begin{table}[]
    \centering
    \begin{tabular}{lllll}
    \toprule
    training method & F1 & Precision & Recall & Test Loss \\
    \midrule
    end-to-end & $0.8376 \pm 0.0100$ & $0.8437 \pm 0.0045$ & $0.8488 \pm 0.0166$ & $0.3379 \pm 0.0510$ \\
    \midrule
    naive MOCO & $0.8380 \pm 0.0007$ & $0.8377 \pm 0.0024$ & $0.8419 \pm 0.0053$ & $0.3781 \pm 0.0226$\\
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{Comparison between naive MOCO classification performance and }
    \label{tab:MOCO_final_metric}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{llllll}
    \toprule
    Classification Head & F1 & Precision & Recall & Test Loss \\
    Dimensions & & & & \\
    \midrule
    layer 1: $2048 \to 9$ & $0.8380$ & $0.8377$ & $0.8419$ & $0.3781$\\
    (linear classification)\\
    \midrule
    layer 1: $2048 \to 512$ & $0.8332$ & $0.8318$ & $0.8399$ & $0.3637$ \\
    layer 2: $512 \to 9$ \\
    \midrule
    layer 1: $2048 \to 1024$ & $0.8388$ & $0.8363$ & $0.8456$ & $0.3369$\\
    layer 2: $1024 \to 9$ \\
    \midrule
    layer 1: $2048 \to 1024$ & $0.8215$ & $0.8160$ & $0.8353$ & $0.3732$\\
    layer 2: $1024 \to 512$ \\
    layer 3: $1024 \to 9$ \\
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{Classification performance of naive MOCO trained encoder with different classification head complexity}
    \label{tab:MOCO_multimlp}
\end{table}

Finally, we present the embedding distribution and confusion matrix learned by naive MOCO learning. We select a model with 0.8379 F1 score trained using the above hyperparameters and epoch numbers. Figure\ref{fig:moco_embedding} shows the t-SNE and PCA embedding visualization. In comparison with the supervised trained models' t-SNE graph, naive MOCO learning also has difficulty distinguishing the cancer-associated stroma and colorectal adenocarcinoma epithelium cells. When PCA dimensionality reduction is used, the model shows significantly worse performance in encoding different classes of images with different embeddings. Apart from the adipose cells, all the rest image categories are encoded to an embedding region that significantly overlaps with at least one other image category's embedding region. \\

The confusion matrix of the same model is shown in Figure \ref{fig:naiveMOCO_cm}, and the per-class precision and recall values are shown in Table \ref{tab:naiveMOCO_preci_recall}. In comparison with the model trained end-to-end on the dev-training set, naive MOCO improves the model performance by around 0.04 precision scores in 5 of the 9 classes and decreases the score by more than 0.1 in NCM and CAS categories. All categories' recall values decreased apart from the NCM and CAS classes, in which naive MOCO training improves the recall value by 0.0894 and 0.3619 respectively. 

\begin{figure}
\centering
    \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=1.0\linewidth]{figures/76227.png}
      \caption{t-SNE embedding visualization}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=1.0\linewidth]{figures/76227_pca.png}
      \caption{PCA embedding visualization}
    \end{subfigure}
    \caption{Embedding visualizations of the model trained with naive MOCO learning}
    \label{fig:moco_embedding}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/76305_cm.png}
    \caption{Confusion Matrix of the model trained with naive MOCO learning}
    \label{fig:naiveMOCO_cm}
\end{figure}

\begin{table}[]
    \centering
    \begin{tabular}{llll}
\toprule
 & adipose & background & debris \\
\hline
Precision & $0.9748 \pm 0.0118$ ($-0.0077$) & $0.9673 \pm 0.0055$ ($0.0262$) & $0.6265 \pm 0.1046$ ($-0.0794$) \\
Recall & $0.9645 \pm 0.0069$ ($0.0004$) & $0.9965 \pm 0.0017$ ($-0.0035$) & $0.8466 \pm 0.0751$ ($0.0324$) \\
\toprule
 & lymphocytes & mucus & smooth muscle \\
\hline
Precision & $0.9736 \pm 0.0024$ ($0.0589$) & $0.9561 \pm 0.0243$ ($-0.0223$) & $0.7421 \pm 0.0253$ ($0.0491$) \\
Recall & $0.8407 \pm 0.0446$ ($-0.1404$) & $0.8706 \pm 0.0336$ ($-0.0509$) & $0.6320 \pm 0.0299$ ($-0.2733$) \\
\toprule
 & NCM & CAS & CAE \\
\hline
Precision & $0.8419 \pm 0.0393$ ($-0.1300$) & $0.4646 \pm 0.0356$ ($-0.2229$) & $0.9186 \pm 0.0295$ ($0.0756$) \\
Recall & $0.9146 \pm 0.0402$ ($0.1179$) & $0.5881 \pm 0.0505$ ($0.3262$) & $0.8785 \pm 0.0293$ ($-0.0646$) \\
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{Per-class precision and recall values of the model trained using naive MOCO learning. The values in the brackets show the difference in metric scores from the model trained end-to-end on the dev-training dataset. }
    \label{tab:naiveMOCO_preci_recall}
\end{table}



\section{Supervised MOCO training} \label{sec:supervisedMOCO_eval}
In this session, we present the hyperparameter tuning results on the learning rates and optimizer. The rest untuned hyperparameters remain unchanged from the naive MOCO training. We then compare the embedding distribution learned by the optimum hyperparameter with that learned by the end-to-end supervised learning.\\

The same learning rate scheduling method from naive MOCO training is used in the supervised MOCO training, where multiple stages of cosine learning rate annealing are used and the initial learning rate at the start of each stage is tuned. Each annealing stage lasts for 20 epochs. Figure \ref{fig:supervisedMOCO_pretrain_curve} shows the training and validation loss in the pretraining stage. The loss curves follow a linear decreasing trend in general, with 0.01 learning rate showing both the lowest final training and validation contrastive losses. We also notice that the training and validation losses increase significantly after the 19th epoch. Similar learning curve patterns are also present in the gradient-based MOCO training, as shown in Figure .... Considering the learning rate is the only parameter changed across different epochs, we plot the learning rates as shown by the red line in the graph. However, the learning rates in the final epochs are set to low values (0.006), which should result in a small change in the loss values. We fail to identify the exact cause of the increase in loss values and leave for further work to explore. 
In all the following experiments, early stopping is used to save the encoder with the lowest contrastive validation loss, before classification heads are trained on the embeddings extracted by the encoder. \\

Figure \ref{fig:supervisedMOCO_mlp_curve} shows the training curves of the classification heads trained on each encoder learned with different learning rates, and Table \ref{tab:supervised_moco_lr} shows the final training and validation losses of these projection heads. The classification head training converges after around 35 epochs of training. A learning rate of 0.01 results in the lowest validation loss and thus is selected as the final learning rate hyperparameter.\\

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figures/supervisedMOCO_pretrain_curve.png}
    \caption{Supervised MOCO pretraining learning curve. The losses follow a linear decreasing trend before epoch 19.}
    \label{fig:supervisedMOCO_pretrain_curve}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figures/supervisedMOCO_mlp_curve.png}
    \caption{Supervised MOCO classification head training curves}
    \label{fig:supervisedMOCO_mlp_curve}
\end{figure}


In addition to the learning rate, we experimented with using an SGD optimizer. As shown in Table \ref{tab:supervised_moco_optimizer}, learning rate 0.01 achieves the lowest validation loss value 0.6973 when SGD is used. However, this validation loss is still significantly worse than that achieved by an Adam optimizer. As a result, we select the Adam optimizer with a learning rate of 0.01 as our final hyperparameter configuration. \\

\begin{minipage}[c]{0.49\textwidth}
    \centering
    \begin{tabular}{lll}
    \toprule
    LR & Training Loss & Validation Loss \\
    \midrule
    0.005 & 0.5056 & 0.2823 \\
    0.01 & 0.4921 & 0.2466 \\
    0.05 & 0.4817 & 0.2541 \\
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{Learning rate hyperparameter tuning results for supervised MOCO training}
    \label{tab:supervised_moco_lr}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
    \centering
    \begin{tabular}{llll}
    \toprule
    LR & Training & Validation Loss \\
    \midrule
    0.01 (Adam) & 0.4921 & 0.2466 \\
    \midrule
    0.005 & 1.6745 & 0.8318 \\
    0.01  & 1.263 & 0.6973 \\
    0.05  & 1.7027 & 0.8511 \\
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{Optimizer hyperparameter tuning for supervised MOCO training}
    \label{tab:supervised_moco_optimizer}
\end{minipage}

Based on the hyperparameters selected as above, we scaled up training to 2 learning rate annealing stages, which consist of 40 epochs in total. Note that in the second stage, we continue training on the model trained for the full 20 epochs in the first stage, instead of the model early stopped at the 19th epoch. Figure \ref{fig:supervisedMOCO_pretraining_final_curve} shows the training curves of the supervised MOCO. As shown by the blue line, the model achieved further improvement in the second cosine annealing stage. In addition, as shown in Table \ref{tab:supervisedMOCO_multistage} and the orange line in Figure \ref{fig:supervisedMOCO_pretraining_final_curve}, training supervisedMOCO for more than 2 stages results in a decrease in the final validation loss. As a result, we report the final performance based on the 2 stages' training. The metrics scores achieved by 3 repeated experiments are shown in Table \ref{tab:supervisedMOCO_final}. In comparison with the metric scores achieved by end-to-end training, no significant difference is shown in the F1, Recall scores, and test set losses. The end-to-end trained model achieved a slightly higher Precision score ($0.8943$ compared to $0.8870$). \\

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figures/supervisedMOCO_pretrain_scaleup.png}
    \caption{Supervised MOCO training curve for more epochs}
    \label{fig:supervisedMOCO_pretraining_final_curve}
\end{figure}

\begin{table}[]
    \centering
    \begin{tabular}{lll}
    \toprule
    LR annealing stage number & F1 score \\
    \midrule
    1 & 0.8367 \\
    2 & 0.8806 \\
    3 & 0.8742 \\
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{LR annealing stage number hyperparameter tuning results for naive MOCO training. A single LR annealing stage consists of 20 epochs of training}
    \label{tab:supervisedMOCO_multistage}
\end{table}



\begin{table}[]
    \centering
    \begin{tabular}{lllll}
    \toprule
    training method & F1 & Precision & Recall & Test Loss \\
    \midrule
    end-to-end & $0.8862 \pm 0.0047$ & $0.8943 \pm 0.0011$ & $0.8919 \pm 0.0044$ & $0.2533 \pm 0.0021$ \\
    \midrule
    supervised MOCO & $0.8833 \pm 0.0038$ & $0.8870 \pm 0.0008$ & $0.8906 \pm 0.0025$ & $0.2534 \pm 0.0033$\\
    \bottomrule
    \end{tabular}
    \captionsetup{type=table}\captionof{table}{Comparison between supervised MOCO and end-to-end supervised learning's metric scores}
    \label{tab:supervisedMOCO_final}
\end{table}

Qualitatively, we compare the embedding distribution learned by supervised MOCO training with end-to-end supervised learning. Figure \ref{fig:supervisedMOCO_embedding} shows the t-SNE and PCA visualization of the embedding distribution learned by the supervised MOCO learning. In general, the distribution differs significantly from that learned by end-to-end training(Figure\ref{fig:large_supervised_embedding}). The t-SNE visualization shows that the model encodes samples belonging to different classes with more clear boundaries. The PCA visualization shows that the supervised MOCO encodes samples from the same category with feature vectors in the same direction, and encodes samples from different categories with vectors orthogonal to each other. The phenomenon shown in the PCA visualization is expected, since the loss function is defined to train the model to encode samples belonging to the same class with a normalized dot product value of 1 and samples from different classes with a normalized dot product value of 0. \\

In addition, we report the same model's confusion matrix and per-class metric scores in Figure \ref{fig:supervisedMOCO_cm} and Table \ref{tab:supervisedMOCO_preci_recall}. ...

\begin{figure}
\centering
    \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=1.0\linewidth]{figures/75349.png}
      \caption{t-SNE embedding visualization}
      \label{fig:supervisedMOCO_tsne}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=1.0\linewidth]{figures/75349_pca.png}
      \caption{PCA embedding visualization}
      \label{fig:supervisedMOCO_pca}
    \end{subfigure}
    \caption{Embedding visualizations of the model trained using supervised MOCO learning}
    \label{fig:supervisedMOCO_embedding}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/75767_cm.png}
    \caption{Confusion Matrix of the model trained using supervised MOCO learning}
    \label{fig:supervisedMOCO_cm}
\end{figure}

\begin{table}[]
    \centering
    \begin{tabular}{llll}
\toprule
 & adipose & background & debris \\
\hline
Precision & $0.9616 \pm 0.0183$ ($-0.0219$) & $0.9702 \pm 0.0047$ ($0.0142$) & $0.6696 \pm 0.0324$ ($-0.1048$) \\
Recall & $0.9151 \pm 0.0354$ ($-0.0677$) & $1.0000 \pm 0.0000$ ($0.0000$) & $0.8805 \pm 0.0146$ ($-0.0103$) \\
\toprule
 & lymphocytes & mucus & smooth muscle \\
\hline
Precision & $0.9302 \pm 0.0035$ ($-0.0164$) & $0.9836 \pm 0.0027$ ($-0.0091$) & $0.7231 \pm 0.0422$ ($-0.0331$) \\
Recall & $0.9874 \pm 0.0067$ ($0.0095$) & $0.9050 \pm 0.0288$ ($-0.0203$) & $0.8621 \pm 0.0084$ ($-0.0618$) \\
\toprule
 & NCM & CAS & CAE \\
\hline
Precision & $0.9372 \pm 0.0135$ ($0.0254$) & $0.7714 \pm 0.0012$ ($-0.0132$) & $0.9261 \pm 0.0030$ ($-0.0242$) \\
Recall & $0.9004 \pm 0.0201$ ($-0.0522$) & $0.5262 \pm 0.0135$ ($0.0405$) & $0.9423 \pm 0.0046$ ($0.0260$) \\
    \bottomrule
    \end{tabular}
    \caption{Per-class precision and recall values of the model trained using supervised MOCO learning}
    \label{tab:supervisedMOCO_preci_recall}
\end{table}


\section{Gradient-based perturbation MOCO} \label{sec:gradientMOCO_eval}
In order to achieve a fair comparison, we 

\chapter{Conclusion and further work}

In this project, we evaluated the difference in embedding quality between the supervised-trained model and the model trained using naive MOCO, supervised MOCO and gradient-based perturbation MOCO. In conclusion, .... \\

Due to time and hardware constraints, various further experiments could not be carried out. As a result, we list the problems worth exploring here for future research. 
\begin{itemize}

\item Avoid using two backward propagations in gradient-based perturbation: The gradient-based perturbation method involved two backward propagation steps to calculate the gradient with respect to the input image. The implementation results in significant computation overhead. In contrast, the other implementation is using the output feature maps of each convolution layers in the Resnet-50 model. This implementation does not require backward propagation, but requires more hyperparameter tuning to decide how to merge feature maps from multiple convolution layers into a single 2D attention map. 

\item Evaluating ViT models' attention-based perturbation method: The original attention masking method used ViT model to get attention value between different image local features. Directly using the attention value calculated during the forward pass could also avoid having two backward propagation in the training step.

\item Avoid using shuffled batch normalization: The shuffled batch normalization method is proposed to prevent the model from taking advantage of the batch-specific information in contrastive learning. In contrast, if setting batch normalization modules in the key encoders to evaluation mode, the batch-normalization layers will not record any batch-specific information. Further experiments are required to evaluate if only setting batch normalization layers to evaluation mode is as effective as the shuffled batch normalization proposed in the original work.

\item Evaluating attention-based contrastive learning on more benchmarks: Significant amount of hyperparameter tuning is required in order to achieve a fair comparison between the attention-based contrastive learning and the original contrastive learning method. As a result, in this project, only the Pathology dataset in the MedMNIST benchmark is selected for evaluation. Additional work is required to evaluate the method's effectiveness on other benchmarks or tasks. 

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Evaluation}
% % On training on the PathMNIST dataset of the MedMNIST benchmark, the training and validation error is shown in Fig. ???. There are two worth noting problems in the training graph. First, training and validation errors have spikes of increase after a certain number of training epochs. This might be related to the problem identified in [moco v3], where initial layers of deep learning models receive high update gradients in these epochs, resulting in the later layers receiving significantly different input features and thus causing high prediction error. Secondly, there is a minor difference in training and validation error, especially when the instance discrimination pretext function is used. Further evaluation is required on the quality of learned embedding and if the model suffers from overfitting or underfitting problem. 

% The dataset used for proof of architecture validity is MedMNIST, but is subject to change due to its small image size. In particular, in the MedMNIST dataset, images of size (28 x 28) are provided, significantly smaller than the ViT model's default input image size, which is (224 x 224). As a result, medical image classification datasets with larger image size is required. \\

% Various evaluation metrics are available for comparing the contrastive learning algorithms' performance. Two qualitative evaluation metrics are used. First, visualization of the embedding distribution is required to evaluate if the model correctly grouped images with high visual similarity. PCA and t-SNE algorithms are effective visualization tools. In addition, the GradCAM metric, which shows image regions that contributed the most to the classification output is also an important metric to evaluate if the model attended to the correct image region when making a prediction. \\

% Quantitatively, the quality of embedding learned is evaluated by performing classification tasks. The classification model could be a multi-layer perception model trained from scratch based on the embeddings extracted by the encoder. In addition, a parameter-free classifier (e.g. K-mean), might also be used. The parameter-free classifier might result in worse performance compared to the parameterized multi-layer perception model. However, it removes the requirement of training an additional model, which provides more consistent evaluation metrics for different embeddings. As a result, both categories of evaluation models are valid solutions. \\

% Finally, the machine learning method's performance on other datasets in the MedMNIST benchmark is also an important evaluation metric. Performing evaluation on other datasets verifies the training method's transferability to other medical image classification tasks. In addition to supervised contrastive learned models, general supervised trained models reported in prior works with similar model complexity are also used as evaluation baselines. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Conclusion}


\bibliographystyle{unsrt}
\bibliography{references}  

\end{document}