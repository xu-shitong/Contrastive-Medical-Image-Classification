{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytSfqn__OmLc"
      },
      "source": [
        "# Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rhGVfICOmLg",
        "outputId": "2962d622-e719-4631-d7d9-a05d248392f2"
      },
      "outputs": [],
      "source": [
        "WORKING_ENV = 'LOCAL' #Â Can be LABS, COLAB or PAPERSPACE\n",
        "assert WORKING_ENV in ['LABS', 'COLAB', 'LOCAL']\n",
        "\n",
        "import sys\n",
        "if WORKING_ENV == 'COLAB':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "  !pip install medmnist\n",
        "  !pip install torch\n",
        "  !pip install gputil\n",
        "  !pip install psutil\n",
        "  !pip install humanize\n",
        "  ROOT = \"/content/drive/MyDrive/ColabNotebooks/med-contrastive-project/\"\n",
        "  sys.path.append(ROOT + \"./swav/\")\n",
        "  !nvidia-smi\n",
        "elif WORKING_ENV == 'LABS':\n",
        "  ROOT = \"/vol/bitbucket/sx119/Contrastive-Medical-Image-Classification/\"\n",
        "else:\n",
        "  ROOT = \"/Users/xushitong/Contrastive-Medical-Image-Classification/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK_mMlhOOmLh"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "EjGT6B2HOmLi"
      },
      "outputs": [],
      "source": [
        "import medmnist\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "from logging import getLogger\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import tqdm\n",
        "# import loader\n",
        "\n",
        "from utils import (\n",
        "    bool_flag,\n",
        "    initialize_exp,\n",
        "    restart_from_checkpoint,\n",
        "    fix_random_seeds,\n",
        "    AverageMeter,\n",
        "    init_distributed_mode,\n",
        ")\n",
        "from multicropdataset import MultiCropDataset\n",
        "import resnet50 as resnet_models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFrMkUBmae_Q",
        "outputId": "004afcb0-c2d8-453b-f6ee-302adc3c420d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU RAM Free: 8.7 GB\n",
            "GPU 0 ... Mem Free: 6329MB / 15360MB | Utilization  57%\n"
          ]
        }
      ],
      "source": [
        "# Import packages\n",
        "import os,sys,humanize,psutil,GPUtil\n",
        "\n",
        "# Define function\n",
        "def mem_report():\n",
        "  print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\n",
        "  \n",
        "  GPUs = GPUtil.getGPUs()\n",
        "  for i, gpu in enumerate(GPUs):\n",
        "    print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
        "\n",
        "mem_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0m-EKEIOmLj"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Kr7c32kOmLj",
        "outputId": "3b461ffe-efe2-4c71-a680-a626b13b5ca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running command ['--epochs', '20', '--batch_size', '16', '--base_lr', '4.8', '--nmb_crops', '2', '--size_crops', '224', '--min_scale_crops', '0.14', '--max_scale_crops', '1', '--data_path', '/Users/xushitong/Contrastive-Medical-Image-Classification/./datasets']\n"
          ]
        }
      ],
      "source": [
        "EPOCH_NUM = 20\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 4.8\n",
        "TRAIN_SET_RATIO = 0.9\n",
        "CROP_NUM = [2]\n",
        "CROP_SIZE = [224]\n",
        "MIN_SCALE_CROP = [0.14]\n",
        "MAX_SCALE_CROP = [1]\n",
        "\n",
        "trial_name = f\"epochs{EPOCH_NUM}_batch{BATCH_SIZE}_lr{LEARNING_RATE}_crop-num{CROP_NUM}-size{CROP_SIZE}_scale-crop-min{MIN_SCALE_CROP}-max{MAX_SCALE_CROP}\"\n",
        "arg_command = \\\n",
        "f\"--epochs {EPOCH_NUM} --batch_size {BATCH_SIZE} --base_lr {LEARNING_RATE}\\\n",
        " --nmb_crops {' '.join(map(str, CROP_NUM))} --size_crops {' '.join(map(str, CROP_SIZE))} --min_scale_crops {' '.join(map(str, MIN_SCALE_CROP))} --max_scale_crops {' '.join(map(str, MAX_SCALE_CROP))}\\\n",
        " --data_path {ROOT}./datasets\".split(\" \")\n",
        "\n",
        "print(f\"Running command {arg_command}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyn7y9gbOmLl",
        "outputId": "c3a4dcb9-1b50-4b09-8b27-8c1421928f14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--seed'], dest='seed', nargs=None, const=None, default=31, type=<class 'int'>, choices=None, required=False, help='seed', metavar=None)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# logger = getLogger()\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Implementation of SwAV\")\n",
        "\n",
        "#########################\n",
        "#### data parameters ####\n",
        "#########################\n",
        "parser.add_argument(\"--data_path\", type=str, default=\"/path/to/data\",\n",
        "                    help=\"path to dataset repository\")\n",
        "parser.add_argument(\"--nmb_crops\", type=int, default=[2], nargs=\"+\",\n",
        "                    help=\"list of number of crops (example: [2, 6])\")\n",
        "parser.add_argument(\"--size_crops\", type=int, default=[224], nargs=\"+\",\n",
        "                    help=\"crops resolutions (example: [224, 96])\")\n",
        "parser.add_argument(\"--min_scale_crops\", type=float, default=[0.14], nargs=\"+\",\n",
        "                    help=\"argument in RandomResizedCrop (example: [0.14, 0.05])\")\n",
        "parser.add_argument(\"--max_scale_crops\", type=float, default=[1], nargs=\"+\",\n",
        "                    help=\"argument in RandomResizedCrop (example: [1., 0.14])\")\n",
        "\n",
        "#########################\n",
        "## swav specific params #\n",
        "#########################\n",
        "parser.add_argument(\"--crops_for_assign\", type=int, nargs=\"+\", default=[0, 1],\n",
        "                    help=\"list of crops id used for computing assignments\")\n",
        "parser.add_argument(\"--temperature\", default=0.1, type=float,\n",
        "                    help=\"temperature parameter in training loss\")\n",
        "parser.add_argument(\"--epsilon\", default=0.05, type=float,\n",
        "                    help=\"regularization parameter for Sinkhorn-Knopp algorithm\")\n",
        "parser.add_argument(\"--sinkhorn_iterations\", default=3, type=int,\n",
        "                    help=\"number of iterations in Sinkhorn-Knopp algorithm\")\n",
        "parser.add_argument(\"--feat_dim\", default=128, type=int,\n",
        "                    help=\"feature dimension\")\n",
        "parser.add_argument(\"--nmb_prototypes\", default=3000, type=int,\n",
        "                    help=\"number of prototypes\")\n",
        "parser.add_argument(\"--queue_length\", type=int, default=0,\n",
        "                    help=\"length of the queue (0 for no queue)\")\n",
        "parser.add_argument(\"--epoch_queue_starts\", type=int, default=15,\n",
        "                    help=\"from this epoch, we start using a queue\")\n",
        "\n",
        "#########################\n",
        "#### optim parameters ###\n",
        "#########################\n",
        "parser.add_argument(\"--epochs\", default=100, type=int,\n",
        "                    help=\"number of total epochs to run\")\n",
        "parser.add_argument(\"--batch_size\", default=64, type=int,\n",
        "                    help=\"batch size per gpu, i.e. how many unique instances per gpu\")\n",
        "parser.add_argument(\"--base_lr\", default=4.8, type=float, help=\"base learning rate\")\n",
        "parser.add_argument(\"--final_lr\", type=float, default=0, help=\"final learning rate\")\n",
        "parser.add_argument(\"--freeze_prototypes_niters\", default=313, type=int,\n",
        "                    help=\"freeze the prototypes during this many iterations from the start\")\n",
        "parser.add_argument(\"--wd\", default=1e-6, type=float, help=\"weight decay\")\n",
        "parser.add_argument(\"--warmup_epochs\", default=10, type=int, help=\"number of warmup epochs\")\n",
        "parser.add_argument(\"--start_warmup\", default=0, type=float,\n",
        "                    help=\"initial warmup learning rate\")\n",
        "\n",
        "#########################\n",
        "#### dist parameters ###\n",
        "#########################\n",
        "parser.add_argument(\"--dist_url\", default=\"env://\", type=str, help=\"\"\"url used to set up distributed\n",
        "                    training; see https://pytorch.org/docs/stable/distributed.html\"\"\")\n",
        "parser.add_argument(\"--world_size\", default=-1, type=int, help=\"\"\"\n",
        "                    number of processes: it is set automatically and\n",
        "                    should not be passed as argument\"\"\")\n",
        "parser.add_argument(\"--rank\", default=0, type=int, help=\"\"\"rank of this process:\n",
        "                    it is set automatically and should not be passed as argument\"\"\")\n",
        "parser.add_argument(\"--local_rank\", default=0, type=int,\n",
        "                    help=\"this argument is not used and should be ignored\")\n",
        "\n",
        "#########################\n",
        "#### other parameters ###\n",
        "#########################\n",
        "parser.add_argument(\"--arch\", default=\"resnet50\", type=str, help=\"convnet architecture\")\n",
        "parser.add_argument(\"--hidden_mlp\", default=2048, type=int,\n",
        "                    help=\"hidden layer dimension in projection head\")\n",
        "parser.add_argument(\"--workers\", default=10, type=int,\n",
        "                    help=\"number of data loading workers\")\n",
        "parser.add_argument(\"--checkpoint_freq\", type=int, default=25,\n",
        "                    help=\"Save the model periodically\")\n",
        "parser.add_argument(\"--use_fp16\", type=bool_flag, default=True,\n",
        "                    help=\"whether to train with mixed precision or not\")\n",
        "parser.add_argument(\"--sync_bn\", type=str, default=\"pytorch\", help=\"synchronize bn\")\n",
        "parser.add_argument(\"--syncbn_process_group_size\", type=int, default=8, help=\"\"\" see\n",
        "                    https://github.com/NVIDIA/apex/blob/master/apex/parallel/__init__.py#L58-L67\"\"\")\n",
        "parser.add_argument(\"--dump_path\", type=str, default=\".\",\n",
        "                    help=\"experiment dump path for checkpoints and log\")\n",
        "parser.add_argument(\"--seed\", type=int, default=31, help=\"seed\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "GYGTw1r1OmLn"
      },
      "outputs": [],
      "source": [
        "args = parser.parse_args(arg_command)\n",
        "fix_random_seeds(args.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY3bZEPVOmLo"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "MWrS62HPOmLo"
      },
      "outputs": [],
      "source": [
        "train_dataset = MultiCropDataset(\n",
        "        medmnist.PathMNIST(\"train\", download=False, root=args.data_path),\n",
        "        args.data_path,\n",
        "        args.size_crops,\n",
        "        args.nmb_crops,\n",
        "        args.min_scale_crops,\n",
        "        args.max_scale_crops,\n",
        "    )\n",
        "\n",
        "pretrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35xqHJCAOmLp"
      },
      "outputs": [],
      "source": [
        "# train_dataset = medmnist.PathMNIST(\"train\", download=False, root=args.data, \n",
        "#                                    transform=loader.TwoCropsTransform(transforms.Compose(augmentation)))\n",
        "# val_dataset = medmnist.PathMNIST(\"val\", download=False, root=args.data, \n",
        "#                                    transform=loader.TwoCropsTransform(transforms.Compose(augmentation)))\n",
        "\n",
        "# pretrain_len = int(len(train_dataset) * TRAIN_SET_RATIO)\n",
        "# pretrain_set, pretrain_val_set = torch.utils.data.random_split(train_dataset, [pretrain_len, len(train_dataset) - pretrain_len])\n",
        "\n",
        "# pretrain_loader = torch.utils.data.DataLoader(\n",
        "#     pretrain_set, batch_size=args.batch_size, shuffle=True, \n",
        "#     pin_memory=True, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvgKDG_WOmLp"
      },
      "source": [
        "# Training Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zToe3Q6tOmLq"
      },
      "outputs": [],
      "source": [
        "if WORKING_ENV == 'LABS':\n",
        "  summary = open(trial_name + \".txt\", \"a\")\n",
        "else:\n",
        "  summary = sys.stdout\n",
        "\n",
        "@torch.no_grad()\n",
        "def distributed_sinkhorn(out):\n",
        "    Q = torch.exp(out / args.epsilon).t() # Q is K-by-B for consistency with notations from our paper\n",
        "    B = Q.shape[1] * args.world_size # number of samples to assign\n",
        "    K = Q.shape[0] # how many prototypes\n",
        "\n",
        "    # make the matrix sums to 1\n",
        "    sum_Q = torch.sum(Q)\n",
        "    # dist.all_reduce(sum_Q)\n",
        "    Q /= sum_Q\n",
        "\n",
        "    for it in range(args.sinkhorn_iterations):\n",
        "        # normalize each row: total weight per prototype must be 1/K\n",
        "        sum_of_rows = torch.sum(Q, dim=1, keepdim=True)\n",
        "        # dist.all_reduce(sum_of_rows)\n",
        "        Q /= sum_of_rows\n",
        "        Q /= K\n",
        "\n",
        "        # normalize each column: total weight per sample must be 1/B\n",
        "        Q /= torch.sum(Q, dim=0, keepdim=True)\n",
        "        Q /= B\n",
        "\n",
        "    Q *= B # the colomns must sum to 1 so that Q is an assignment\n",
        "    return Q.t()\n",
        "\n",
        "\n",
        "def train(train_loader, model, optimizer, epoch, lr_schedule, queue):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    model.train()\n",
        "    use_the_queue = False\n",
        "\n",
        "    end = time.time()\n",
        "    with tqdm.tqdm(pretrain_loader, unit=\"batch\") as tepoch: \n",
        "        if WORKING_ENV == \"LABS\":\n",
        "          tepoch = pretrain_loader\n",
        "        for it, inputs in enumerate(tepoch):\n",
        "            # measure data loading time\n",
        "            data_time.update(time.time() - end)\n",
        "\n",
        "            # update learning rate\n",
        "            iteration = epoch * len(train_loader) + it\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group[\"lr\"] = lr_schedule[iteration]\n",
        "\n",
        "            # normalize the prototypes\n",
        "            with torch.no_grad():\n",
        "                w = model.prototypes.weight.data.clone()\n",
        "                w = nn.functional.normalize(w, dim=1, p=2)\n",
        "                model.prototypes.weight.copy_(w)\n",
        "\n",
        "            # ============ multi-res forward passes ... ============\n",
        "            embedding, output = model(inputs)\n",
        "            embedding = embedding.detach()\n",
        "            bs = inputs[0].size(0)\n",
        "\n",
        "            # ============ swav loss ... ============\n",
        "            loss = 0\n",
        "            for i, crop_id in enumerate(args.crops_for_assign):\n",
        "                with torch.no_grad():\n",
        "                    out = output[bs * crop_id: bs * (crop_id + 1)].detach()\n",
        "\n",
        "                    # time to use the queue\n",
        "                    if queue is not None:\n",
        "                        if use_the_queue or not torch.all(queue[i, -1, :] == 0):\n",
        "                            use_the_queue = True\n",
        "                            out = torch.cat((torch.mm(\n",
        "                                queue[i],\n",
        "                                model.module.prototypes.weight.t()\n",
        "                            ), out))\n",
        "                        # fill the queue\n",
        "                        queue[i, bs:] = queue[i, :-bs].clone()\n",
        "                        queue[i, :bs] = embedding[crop_id * bs: (crop_id + 1) * bs]\n",
        "\n",
        "                    # get assignments\n",
        "                    q = distributed_sinkhorn(out)[-bs:]\n",
        "\n",
        "                # cluster assignment prediction\n",
        "                subloss = 0\n",
        "                for v in np.delete(np.arange(np.sum(args.nmb_crops)), crop_id):\n",
        "                    x = output[bs * v: bs * (v + 1)] / args.temperature\n",
        "                    subloss -= torch.mean(torch.sum(q * F.log_softmax(x, dim=1), dim=1))\n",
        "                loss += subloss / (np.sum(args.nmb_crops) - 1)\n",
        "            loss /= len(args.crops_for_assign)\n",
        "\n",
        "            # ============ backward and optim step ... ============\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # cancel gradients for the prototypes\n",
        "            if iteration < args.freeze_prototypes_niters:\n",
        "                for name, p in model.named_parameters():\n",
        "                    if \"prototypes\" in name:\n",
        "                        p.grad = None\n",
        "            optimizer.step()\n",
        "\n",
        "            # ============ misc ... ============\n",
        "            losses.update(loss.item(), inputs[0].size(0))\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            if args.rank ==0 and it % 50 == 0:\n",
        "                summary.write(\"Epoch: [{0}][{1}]\\t\"\n",
        "                    \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
        "                    \"Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n",
        "                    \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\"\n",
        "                    \"Lr: {lr:.4f}\".format(\n",
        "                        epoch,\n",
        "                        it,\n",
        "                        batch_time=batch_time,\n",
        "                        data_time=data_time,\n",
        "                        loss=losses,\n",
        "                        lr=optimizer.param_groups[0][\"lr\"],\n",
        "                    )\n",
        "                )\n",
        "    return (epoch, losses.avg), queue\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8pT2SzpOmLr"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "Dg-PMLCcOmLt",
        "outputId": "a238a5b2-c4e0-434d-d7ee-8f26bd1dafbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/5625 [00:00<1:00:20,  1.55batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [0][0]\tTime 0.646 (0.646)\tData 0.255 (0.255)\tLoss 8.3943 (8.3943)\tLr: 0.0000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 33/5625 [00:21<59:50,  1.56batch/s]  \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-ff1c7dac71d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;31m# training_stats.update(scores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-be3fc77d10c4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, epoch, lr_schedule, queue)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mWORKING_ENV\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"LABS\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0mtepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrain_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;31m# measure data loading time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/ColabNotebooks/med-contrastive-project/./swav/multicropdataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# image = self.loader(path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mmulti_crops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_crops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/ColabNotebooks/med-contrastive-project/./swav/multicropdataset.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trans)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# image = self.loader(path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mmulti_crops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_crops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1251\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msaturation_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1254\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhue_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36madjust_saturation\u001b[0;34m(img, saturation_factor)\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36madjust_saturation\u001b[0;34m(img, saturation_factor)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0menhancer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEnhance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menhance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaturation_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/ImageEnhance.py\u001b[0m in \u001b[0;36menhance\u001b[0;34m(self, factor)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegenerate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mblend\u001b[0;34m(im1, im2, alpha)\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \u001b[0mim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2938\u001b[0m     \u001b[0mim2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2939\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# logger, training_stats = initialize_exp(args, \"epoch\", \"loss\")\n",
        "\n",
        "# build data\n",
        "# train_dataset = MultiCropDataset(\n",
        "#     args.data_path,\n",
        "#     args.size_crops,\n",
        "#     args.nmb_crops,\n",
        "#     args.min_scale_crops,\n",
        "#     args.max_scale_crops,\n",
        "# )\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     train_dataset,\n",
        "#     batch_size=args.batch_size,\n",
        "#     num_workers=args.workers,\n",
        "#     pin_memory=True,\n",
        "#     drop_last=True\n",
        "# )\n",
        "# logger.info(\"Building data done with {} images loaded.\".format(len(train_dataset)))\n",
        "\n",
        "# build model\n",
        "# model = torchvision.models.resnet50()\n",
        "model = resnet_models.__dict__[args.arch](\n",
        "        normalize=True,\n",
        "        hidden_mlp=args.hidden_mlp,\n",
        "        output_dim=args.feat_dim,\n",
        "        nmb_prototypes=args.nmb_prototypes,\n",
        "    )\n",
        "# # synchronize batch norm layers\n",
        "# model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
        "# copy model to GPU\n",
        "model = model.cuda()\n",
        "# if args.rank == 0:\n",
        "#     logger.info(model)\n",
        "# logger.info(\"Building model done.\")\n",
        "\n",
        "# build optimizer\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=args.base_lr,\n",
        "    momentum=0.9,\n",
        "    weight_decay=args.wd,\n",
        ")\n",
        "# optimizer = LARC(optimizer=optimizer, trust_coefficient=0.001, clip=False)\n",
        "warmup_lr_schedule = np.linspace(args.start_warmup, args.base_lr, len(pretrain_loader) * args.warmup_epochs)\n",
        "iters = np.arange(len(pretrain_loader) * (args.epochs - args.warmup_epochs))\n",
        "cosine_lr_schedule = np.array([args.final_lr + 0.5 * (args.base_lr - args.final_lr) * (1 + \\\n",
        "                      math.cos(math.pi * t / (len(pretrain_loader) * (args.epochs - args.warmup_epochs)))) for t in iters])\n",
        "lr_schedule = np.concatenate((warmup_lr_schedule, cosine_lr_schedule))\n",
        "# logger.info(\"Building optimizer done.\")\n",
        "\n",
        "# # wrap model\n",
        "# model = nn.parallel.DistributedDataParallel(\n",
        "#     model,\n",
        "#     device_ids=[args.gpu_to_work_on]\n",
        "# )\n",
        "\n",
        "# optionally resume from a checkpoint\n",
        "to_restore = {\"epoch\": 0}\n",
        "# restart_from_checkpoint(\n",
        "#     os.path.join(args.dump_path, \"checkpoint.pth.tar\"),\n",
        "#     run_variables=to_restore,\n",
        "#     state_dict=model,\n",
        "#     optimizer=optimizer,\n",
        "#     amp=apex.amp,\n",
        "# )\n",
        "start_epoch = to_restore[\"epoch\"]\n",
        "\n",
        "# build the queue\n",
        "queue = None\n",
        "queue_path = os.path.join(args.dump_path, \"queue\" + str(args.rank) + \".pth\")\n",
        "if os.path.isfile(queue_path):\n",
        "    queue = torch.load(queue_path)[\"queue\"]\n",
        "# the queue needs to be divisible by the batch size\n",
        "args.queue_length -= args.queue_length % (args.batch_size * args.world_size)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "    # train the network for one epoch\n",
        "    # logger.info(\"============ Starting epoch %i ... ============\" % epoch)\n",
        "\n",
        "    # # set sampler\n",
        "    # train_loader.sampler.set_epoch(epoch)\n",
        "\n",
        "    # optionally starts a queue\n",
        "    if args.queue_length > 0 and epoch >= args.epoch_queue_starts and queue is None:\n",
        "        queue = torch.zeros(\n",
        "            len(args.crops_for_assign),\n",
        "            args.queue_length // args.world_size,\n",
        "            args.feat_dim,\n",
        "        ).cuda()\n",
        "\n",
        "    # train the network\n",
        "    scores, queue = train(pretrain_loader, model, optimizer, epoch, lr_schedule, queue)\n",
        "    # training_stats.update(scores)\n",
        "\n",
        "    # # save checkpoints\n",
        "    # if args.rank == 0:\n",
        "    #     save_dict = {\n",
        "    #         \"epoch\": epoch + 1,\n",
        "    #         \"state_dict\": model.state_dict(),\n",
        "    #         \"optimizer\": optimizer.state_dict(),\n",
        "    #     }\n",
        "    #     torch.save(\n",
        "    #         save_dict,\n",
        "    #         os.path.join(args.dump_path, \"checkpoint.pth.tar\"),\n",
        "    #     )\n",
        "    #     if epoch % args.checkpoint_freq == 0 or epoch == args.epochs - 1:\n",
        "    #         shutil.copyfile(\n",
        "    #             os.path.join(args.dump_path, \"checkpoint.pth.tar\"),\n",
        "    #             os.path.join(args.dump_checkpoints, \"ckp-\" + str(epoch) + \".pth\"),\n",
        "    #         )\n",
        "    # if queue is not None:\n",
        "    #     torch.save({\"queue\": queue}, queue_path)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSzm5lRHOmLu"
      },
      "outputs": [],
      "source": [
        "torch.save(model, f\"{trial_name}.pickle\")\n",
        "mem_report()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "med-contrast-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "89c22432502bdc03679fbb2d05029bfff3d90287672507a41449dbd4432b55dc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
