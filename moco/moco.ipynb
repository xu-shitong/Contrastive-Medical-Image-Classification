{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR0Flq1UxNNA"
      },
      "source": [
        "# Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rfzXouQxNNE",
        "outputId": "4943fd56-217a-4404-d4d5-0aa853288d7c"
      },
      "outputs": [],
      "source": [
        "WORKING_ENV = 'LOCAL' #Â Can be LABS, COLAB or PAPERSPACE\n",
        "assert WORKING_ENV in ['LABS', 'COLAB', 'LOCAL']\n",
        "\n",
        "import sys\n",
        "if WORKING_ENV == 'COLAB':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "  !pip install medmnist\n",
        "  !pip install torch\n",
        "  !pip install gputil\n",
        "  !pip install psutil\n",
        "  !pip install humanize\n",
        "  ROOT = \"/content/drive/MyDrive/ColabNotebooks/med-contrastive-project/\"\n",
        "  sys.path.append(ROOT + \"./moco/\")\n",
        "elif WORKING_ENV == 'LABS':\n",
        "  ROOT = \"/vol/bitbucket/sx119/Contrastive-Medical-Image-Classification/\"\n",
        "else:\n",
        "  ROOT = \"/Users/xushitong/Contrastive-Medical-Image-Classification/\"\n",
        "# sys.path.append(ROOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR8ZgATBxNNG"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ygRaiz3zxNNG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/xushitong/miniconda3/envs/med-contrast-env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import medmnist\n",
        "\n",
        "import argparse\n",
        "import builtins\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import tqdm\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "# import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.multiprocessing as mp\n",
        "import torch.utils.data\n",
        "# import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "import loader\n",
        "import builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwggMf_Yw-i7",
        "outputId": "41014f08-c1bb-4468-ecb0-e9b18c817720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU RAM Free: 12.3 GB\n",
            "GPU 0 ... Mem Free: 15109MB / 15109MB | Utilization   0%\n"
          ]
        }
      ],
      "source": [
        "# Import packages\n",
        "import os,sys,humanize,psutil,GPUtil\n",
        "\n",
        "# Define function\n",
        "def mem_report():\n",
        "  print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\n",
        "  \n",
        "  GPUs = GPUtil.getGPUs()\n",
        "  for i, gpu in enumerate(GPUs):\n",
        "    print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
        "\n",
        "mem_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0-zL3BAxNNI"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NSLD_2PmxNNI"
      },
      "outputs": [],
      "source": [
        "EPOCH_NUM = 200\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.03\n",
        "MOMENTUM = 0.9\n",
        "\n",
        "trial_name = f\"epochs{EPOCH_NUM}_batch{BATCH_SIZE}_lr{LEARNING_RATE}_momentum{MOMENTUM}\"\n",
        "arg_command = f\"--epochs_{EPOCH_NUM}_-b_{BATCH_SIZE}_--lr_{LEARNING_RATE}_--momentum_{MOMENTUM}_--print-freq_100_{'' if WORKING_ENV == 'LOCAL' else '--gpu_0_'}{ROOT}./datasets\".split(\"_\")\n",
        "\n",
        "print(f\"Running command {arg_command}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct1yDbRsxNNJ",
        "outputId": "d489fdca-389c-4680-8032-1e138aa66ba5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "_StoreTrueAction(option_strings=['--cos'], dest='cos', nargs=0, const=True, default=False, type=None, choices=None, help='use cosine lr schedule', metavar=None)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "\n",
        "model_names = sorted(name for name in models.__dict__\n",
        "    if name.islower() and not name.startswith(\"__\")\n",
        "    and callable(models.__dict__[name]))\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
        "parser.add_argument('data', metavar='DIR',\n",
        "                    help='path to dataset')\n",
        "parser.add_argument('-a', '--arch', metavar='ARCH', default='resnet50',\n",
        "                    choices=model_names,\n",
        "                    help='model architecture: ' +\n",
        "                        ' | '.join(model_names) +\n",
        "                        ' (default: resnet50)')\n",
        "# parser.add_argument('-j', '--workers', default=32, type=int, metavar='N',\n",
        "#                     help='number of data loading workers (default: 32)')\n",
        "parser.add_argument('--epochs', default=200, type=int, metavar='N',\n",
        "                    help='number of total epochs to run')\n",
        "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
        "                    help='manual epoch number (useful on restarts)')\n",
        "parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
        "                    metavar='N',\n",
        "                    help='mini-batch size (default: 256), this is the total '\n",
        "                         'batch size of all GPUs on the current node when '\n",
        "                         'using Data Parallel or Distributed Data Parallel')\n",
        "parser.add_argument('--lr', '--learning-rate', default=0.03, type=float,\n",
        "                    metavar='LR', help='initial learning rate', dest='lr')\n",
        "parser.add_argument('--schedule', default=[120, 160], nargs='*', type=int,\n",
        "                    help='learning rate schedule (when to drop lr by 10x)')\n",
        "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
        "                    help='momentum of SGD solver')\n",
        "parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\n",
        "                    metavar='W', help='weight decay (default: 1e-4)',\n",
        "                    dest='weight_decay')\n",
        "parser.add_argument('-p', '--print-freq', default=10, type=int,\n",
        "                    metavar='N', help='print frequency (default: 10)')\n",
        "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
        "                    help='path to latest checkpoint (default: none)')\n",
        "# parser.add_argument('--world-size', default=-1, type=int,\n",
        "#                     help='number of nodes for distributed training')\n",
        "# parser.add_argument('--rank', default=-1, type=int,\n",
        "#                     help='node rank for distributed training')\n",
        "# parser.add_argument('--dist-url', default='tcp://224.66.41.62:23456', type=str,\n",
        "#                     help='url used to set up distributed training')\n",
        "# parser.add_argument('--dist-backend', default='nccl', type=str,\n",
        "#                     help='distributed backend')\n",
        "parser.add_argument('--seed', default=None, type=int,\n",
        "                    help='seed for initializing training. ')\n",
        "parser.add_argument('--gpu', default=None, type=int,\n",
        "                    help='GPU id to use.')\n",
        "# parser.add_argument('--multiprocessing-distributed', action='store_true',\n",
        "#                     help='Use multi-processing distributed training to launch '\n",
        "#                          'N processes per node, which has N GPUs. This is the '\n",
        "#                          'fastest way to use PyTorch for either single node or '\n",
        "#                          'multi node data parallel training')\n",
        "\n",
        "# moco specific configs:\n",
        "parser.add_argument('--moco-dim', default=128, type=int,\n",
        "                    help='feature dimension (default: 128)')\n",
        "parser.add_argument('--moco-k', default=65536, type=int,\n",
        "                    help='queue size; number of negative keys (default: 65536)')\n",
        "parser.add_argument('--moco-m', default=0.999, type=float,\n",
        "                    help='moco momentum of updating key encoder (default: 0.999)')\n",
        "parser.add_argument('--moco-t', default=0.07, type=float,\n",
        "                    help='softmax temperature (default: 0.07)')\n",
        "\n",
        "# options for moco v2\n",
        "parser.add_argument('--mlp', action='store_true',\n",
        "                    help='use mlp head')\n",
        "parser.add_argument('--aug-plus', action='store_true',\n",
        "                    help='use moco v2 data augmentation')\n",
        "parser.add_argument('--cos', action='store_true',\n",
        "                    help='use cosine lr schedule')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FvLjiq92mAw1"
      },
      "outputs": [],
      "source": [
        "args = parser.parse_args(arg_command)\n",
        "\n",
        "if args.seed is not None:\n",
        "    random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    cudnn.deterministic = True\n",
        "    warnings.warn('You have chosen to seed training. '\n",
        "                  'This will turn on the CUDNN deterministic setting, '\n",
        "                  'which can slow down your training considerably! '\n",
        "                  'You may see unexpected behavior when restarting '\n",
        "                  'from checkpoints.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuYf0uhUmAw2"
      },
      "source": [
        "# Training helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "a1wqf6--mAw3"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, file, num_batches, meters, prefix=\"\"):\n",
        "        self.file = file\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        self.file.write('\\t'.join(entries) + \"\\n\")\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
        "    lr = args.lr\n",
        "    if args.cos:  # cosine lr schedule\n",
        "        lr *= 0.5 * (1. + math.cos(math.pi * epoch / args.epochs))\n",
        "    else:  # stepwise lr schedule\n",
        "        for milestone in args.schedule:\n",
        "            lr *= 0.1 if epoch >= milestone else 1.\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def update_accuracy_meters(losses, top1, top5, output, target, loss, step_size):\n",
        "    \"\"\"Update loss, top1, top5 metrics for either train or validation\n",
        "    Inputs:\n",
        "      - step_size: parameter n for loss/top1/top5 meters\n",
        "    \"\"\"\n",
        "    # acc1/acc5 are (K+1)-way contrast classifier accuracy\n",
        "    # measure accuracy and record loss\n",
        "    acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "    losses.update(loss.item(), step_size)\n",
        "    top1.update(acc1[0], step_size)\n",
        "    top5.update(acc5[0], step_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jKX_SGOmAw4"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kVr5uUDQmAw4"
      },
      "outputs": [],
      "source": [
        "# Data loading code\n",
        "# traindir = os.path.join(args.data, 'train')\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "if args.aug_plus:\n",
        "    # MoCo v2's aug: similar to SimCLR https://arxiv.org/abs/2002.05709\n",
        "    augmentation = [\n",
        "        transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
        "        ], p=0.8),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.RandomApply([loader.GaussianBlur([.1, 2.])], p=0.5),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]\n",
        "else:\n",
        "    # MoCo v1's aug: the same as InstDisc https://arxiv.org/abs/1805.01978\n",
        "    augmentation = [\n",
        "        transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]\n",
        "\n",
        "train_dataset = medmnist.PathMNIST(\"train\", download=False, root=args.data, \n",
        "                                   transform=loader.TwoCropsTransform(transforms.Compose(augmentation)))\n",
        "val_dataset = medmnist.PathMNIST(\"val\", download=False, root=args.data, \n",
        "                                 transform=loader.TwoCropsTransform(transforms.Compose(augmentation)))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=args.batch_size, shuffle=True, \n",
        "    pin_memory=True, drop_last=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=2 * args.batch_size, shuffle=False, \n",
        "    pin_memory=True, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnG87FNTxNNL"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKBaxhUMmAw8",
        "outputId": "b5c6907d-7c87-4468-c48a-c8837cbfd1ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> creating model 'resnet50'\n"
          ]
        }
      ],
      "source": [
        "if WORKING_ENV == 'LABS':\n",
        "  summary = open(trial_name + \".txt\", \"a\")\n",
        "else:\n",
        "  summary = sys.stdout\n",
        "\n",
        "print(\"=> creating model '{}'\".format(args.arch))\n",
        "model = builder.MoCo(\n",
        "    models.__dict__[args.arch],\n",
        "    args.moco_dim, args.moco_k, args.moco_m, args.moco_t, args.mlp)\n",
        "# print(model)\n",
        "\n",
        "if args.gpu is not None:\n",
        "  torch.cuda.set_device(args.gpu)\n",
        "  model = model.cuda(args.gpu)\n",
        "  criterion = nn.CrossEntropyLoss().cuda(args.gpu)\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
        "                            momentum=args.momentum,\n",
        "                            weight_decay=args.weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "bxWMJP9cmAxA",
        "outputId": "0887a631-5915-43e1-84bd-a26f1bfa0d84"
      },
      "outputs": [],
      "source": [
        "if args.gpu is not None:\n",
        "  cudnn.benchmark = True\n",
        "\n",
        "for epoch in range(args.start_epoch, args.epochs):\n",
        "    adjust_learning_rate(optimizer, epoch, args)\n",
        "\n",
        "    # train for one epoch\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    data_time = AverageMeter('Data', ':6.3f')\n",
        "    losses = AverageMeter('TrainLoss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    val_losses = AverageMeter('ValLoss', ':.4e')\n",
        "    val_top1 = AverageMeter('ValAcc@1', ':6.2f')\n",
        "    val_top5 = AverageMeter('ValAcc@5', ':6.2f')\n",
        "    progress = ProgressMeter(\n",
        "        summary,\n",
        "        len(train_loader),\n",
        "        [batch_time, data_time, losses, top1, top5, val_losses, val_top1, val_top5],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    with tqdm.tqdm(train_loader, unit=\"batch\") as tepoch: \n",
        "      if WORKING_ENV == \"LABS\":\n",
        "        tepoch = train_loader\n",
        "      for i, (images, _) in enumerate(tepoch):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        if args.gpu is not None:\n",
        "            images[0] = images[0].cuda(args.gpu, non_blocking=True)\n",
        "            images[1] = images[1].cuda(args.gpu, non_blocking=True)\n",
        "\n",
        "        # compute output\n",
        "        output, target = model(im_q=images[0], im_k=images[1])\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        update_accuracy_meters(losses, top1, top5, output, target, loss, images[0].size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if not WORKING_ENV == 'LABS':\n",
        "          tepoch.set_description(f\"batch {i}\")\n",
        "          tepoch.set_postfix(\n",
        "                            loss=loss.val,\n",
        "                            top1=top1.val,\n",
        "                            top5=top5.val)\n",
        "\n",
        "        if i % args.print_freq == 0 and not i == 0:\n",
        "          with torch.no_grad():\n",
        "            model.eval()\n",
        "            # evaluate on validation set\n",
        "            for (images, _) in val_loader:\n",
        "              if args.gpu is not None:\n",
        "                images[0] = images[0].cuda(args.gpu, non_blocking=True)\n",
        "                images[1] = images[1].cuda(args.gpu, non_blocking=True)\n",
        "              output, target = model(im_q=images[0], im_k=images[1], train=False)\n",
        "              loss = criterion(output, target)\n",
        "              update_accuracy_meters(val_losses, val_top1, val_top5, output, target, loss, images[0].size(0))\n",
        "            \n",
        "            model.train()\n",
        "          \n",
        "          progress.display(i)\n",
        "\n",
        "\n",
        "    # if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n",
        "    #         and args.rank % ngpus_per_node == 0):\n",
        "    #     save_checkpoint({\n",
        "    #         'epoch': epoch + 1,\n",
        "    #         'arch': args.arch,\n",
        "    #         'state_dict': model.state_dict(),\n",
        "    #         'optimizer' : optimizer.state_dict(),\n",
        "    #     }, is_best=False, filename='checkpoint_{:04d}.pth.tar'.format(epoch))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if WORKING_ENV == 'LABS':\n",
        "  summary.close()\n",
        "\n",
        "torch.save(model, \"model.pickle\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejL5KMHcvoIG",
        "outputId": "45b4188d-b166-4f7b-c120-1677c3f47146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU RAM Free: 10.4 GB\n",
            "GPU 0 ... Mem Free: 10315MB / 15109MB | Utilization  32%\n"
          ]
        }
      ],
      "source": [
        "mem_report()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "med-contrast-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "89c22432502bdc03679fbb2d05029bfff3d90287672507a41449dbd4432b55dc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
