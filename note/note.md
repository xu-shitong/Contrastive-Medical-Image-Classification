# MOCO
- simCLR: use tpu, has large batch size
- memory bank: extract all feature, momentum update key from each batch
- moco: queue to keep a smaller memory bank, momentum update encoder
pretask: Use instance discrimination, e.g. different crop from the same image. 

contrastive loss: target is extracted using the other model

simCLR: use tpu, has large 

official moco and moco-v2 code: https://github.com/facebookresearch/moco/blob/main/main_moco.py

official moco v3: https://github.com/facebookresearch/moco-v3

official SwAV code: https://github.com/facebookresearch/swav

official SimCLR code: https://github.com/google-research/simclr

|        | MOCO   | MOCO v2 | SimCLR | SwAV | MoCo v3
|  ----  |  ----  |  ----   |  ----  |  ----  | ---- |
| pos pair generation method | random resize crop, pair generated by different encoder | similar to moco, model added mlp layer | same image argumentation generate positive pair, random-resize-crop gaussian-blur colour-jittering, showed adding mlp layer after encoder improves performance | use Multi-crop, 2 (160, 160) crops and V (96, 96) crops | similar to SimCLR
| batch size | 256 for ImageNet-1M/ 1024 for Instagram-1B | 256 | 256 to 8192 before argumentation | achieve similar result with MoCo and Simclr with smaller queue size, learn faster (200 epoch to achieve MoCo 800 epoch result) training time longer than MoCo due to additional backward propoagation | 4096, no memory queue
| computation resource | 8 GPU 50 hour/ 64 GPU for 6 day | 5G GPU memory | TPU 1.5hour 93G GPU memory using 4096 batch size | 64 V100 GPU, 16GB memory | used ViT, ViT-B on public GCP TPUv3 takes 6.3 hour for 300 epoch

GPU requirement for 

pathMnist

one method one dataset practical result